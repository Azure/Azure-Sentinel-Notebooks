{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hunting - Automated Data Query and MDTI API and Ingestion to Custom Table\n",
        "\n",
        "__Notebook Version:__ 1.0<br>\n",
        "__Python Version:__ Python 3.8<br>\n",
        "__Apache Spark Version:__ 3.1<br>\n",
        "__Required Packages:__ azure-monitor-query, azure-mgmt-loganalytics<br>\n",
        "__Platforms Supported:__  Azure Synapse Analytics\n",
        "     \n",
        "__Data Source Required:__ Log Analytics custom table defined\n",
        "    \n",
        "### Description\n",
        "This notebook provides step-by-step instructions and sample code to query various data from Azure Log Analytics and then store it back to Log Analytocs pre-defined custom table.<br>\n",
        "*** Please run the cells sequentially to avoid errors.  Please do not use \"run all cells\". *** <br>\n",
        "Need to know more about KQL? [Getting started with Kusto Query Language](https://docs.microsoft.com/azure/data-explorer/kusto/concepts/).\n",
        "\n",
        "## Table of Contents\n",
        "1. Warm-up\n",
        "2. Azure Log Analytics Data Queries\n",
        "3. Save result to Azure Log Analytics Custom Table"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Warm-up"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install  azure.mgmt.loganalytics"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure.monitor.query"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure.monitor.ingestion"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Python libraries that will be used in this notebook\n",
        "from azure.mgmt.loganalytics import LogAnalyticsManagementClient\n",
        "from azure.monitor.query import LogsQueryClient, MetricsQueryClient, LogsQueryStatus\n",
        "##from azure.identity.aio import DefaultAzureCredential\n",
        "from azure.monitor.ingestion import LogsIngestionClient\n",
        "\n",
        "from azure.identity import AzureCliCredential, DefaultAzureCredential, ClientSecretCredential\n",
        "from azure.core.exceptions import  HttpResponseError \n",
        "\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import json\n",
        "import ipywidgets\n",
        "from IPython.display import display, HTML, Markdown"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tenant_id = ''\r\n",
        "subscription_id = ''\r\n",
        "akv_name = ''\r\n",
        "akv_link_name = ''\r\n",
        "workspace_id = ''\r\n",
        "client_id_name = ''\r\n",
        "client_secret_name = ''\r\n",
        "workspace_id_for_query = ''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for provisioning resources\r\n",
        "resource_group_name = \"\"\r\n",
        "location = \"\"\r\n",
        "workspace_name = ''\r\n",
        "workspace_resource_id = '/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.OperationalInsights/workspaces/{2}'.format(subscription_id, resource_group_name, workspace_name)\r\n",
        "data_collection_endpoint_name = \"\"\r\n",
        "data_collection_rule_name = \"\"\r\n",
        "stream_name = \"\"\r\n",
        "immutable_rule_id = \"\"\r\n",
        "dce_endpoint = ''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to change resource_uri for various cloud environments.\r\n",
        "resource_uri = \"https://api.loganalytics.io\"\r\n",
        "client_id = mssparkutils.credentials.getSecret(akv_name, client_id_name, akv_link_name)\r\n",
        "client_secret = mssparkutils.credentials.getSecret(akv_name, client_secret_name, akv_link_name)\r\n",
        "\r\n",
        "credential = ClientSecretCredential(\r\n",
        "    tenant_id=tenant_id, \r\n",
        "    client_id=client_id, \r\n",
        "    client_secret=client_secret)\r\n",
        "access_token = credential.get_token(resource_uri + \"/.default\")\r\n",
        "token = access_token[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Azure Log Analytics Data Queries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for query\r\n",
        "\r\n",
        "def query_la(workspace_id, query):\r\n",
        "    la_data_client = LogsQueryClient(credential=credential)\r\n",
        "    end_time =  datetime.now(timezone.utc)\r\n",
        "    start_time = end_time - timedelta(15)\r\n",
        "\r\n",
        "    query_result = la_data_client.query_workspace(\r\n",
        "        workspace_id=workspace_id,\r\n",
        "        query=query,\r\n",
        "        timespan=(start_time, end_time))\r\n",
        "    \r\n",
        "    df_la_query = pd.DataFrame\r\n",
        "\r\n",
        "    if query_result.status == LogsQueryStatus.SUCCESS:\r\n",
        "        data = query_result.tables\r\n",
        "        if len(query_result.tables) > 1:\r\n",
        "            print('You have more than one tyable to processs')\r\n",
        "    elif query_result.status == LogsQueryStatus.PARTIAL:\r\n",
        "        data=query_result.partial_data\r\n",
        "        print(query_result.partial_error)\r\n",
        "    else:\r\n",
        "        print(query_result.error)\r\n",
        "    \r\n",
        "    if len(query_result.tables) > 1:\r\n",
        "        print('You have more than one tyable to processs')\r\n",
        "    for table in data:\r\n",
        "        df_la_query = pd.DataFrame(data=table.rows, columns=table.columns)\r\n",
        "        return df_la_query\r\n",
        "\r\n",
        "def query_la_for_ip(df_merge, query):\r\n",
        "    la_data_client = LogsQueryClient(credential=credential)\r\n",
        "    end_time =  datetime.now(timezone.utc)\r\n",
        "    start_time = end_time - timedelta(15)\r\n",
        "\r\n",
        "    query = \"CommonSecurityLog | where TimeGenerated > ago({0}) | where DestinationIP contains '{1}' | project DestinationIP\".format(lookback_period, ip_prefix)\r\n",
        "    query_result = la_data_client.query_workspace(\r\n",
        "        workspace_id=workspace_id_for_query,\r\n",
        "        query=query,\r\n",
        "        timespan=(start_time, end_time))\r\n",
        "\r\n",
        "    if query_result.status == LogsQueryStatus.SUCCESS:\r\n",
        "        df_la_query = pd.DataFrame(data=query_result.tables[0].rows, columns=query_result.tables[0].columns)\r\n",
        "        print(df_la_query.count())\r\n",
        "        return df_la_query\r\n",
        "\r\n",
        "def create_ip_prefix(base):\r\n",
        "    a = []\r\n",
        "    for i in range(1,10):\r\n",
        "        a.append(base + str(i))\r\n",
        "    return a"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Slice data for query"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.DataFrame()\r\n",
        "ip_prefix_array = create_ip_prefix('10.68.88.')\r\n",
        "lookback_period = '4d'\r\n",
        "\r\n",
        "for ip_pre in ip_prefix_array:\r\n",
        "    query = \"CommonSecurityLog | where TimeGenerated > ago({0}) | where DestinationIP contains '{1}' | project DestinationIP\".format(lookback_period, ip_pre)\r\n",
        "    #print(query)\r\n",
        "    df_la_query = query_la(workspace_id_for_query, query)\r\n",
        "    df_final = pd.concat([df_final, df_la_query])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_final)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Multi-processing"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_la_for_multi(df_merge, lookback_days, ip_prefix):\r\n",
        "    la_data_client = LogsQueryClient(credential=credential)\r\n",
        "    end_time =  datetime.now(timezone.utc)\r\n",
        "    start_time = end_time - timedelta(15)\r\n",
        "\r\n",
        "    query = \"CommonSecurityLog | where TimeGenerated > ago({0}) | where DestinationIP contains '{1}' | project DestinationIP\".format(lookback_days, ip_prefix)\r\n",
        "    #print(query)\r\n",
        "    query_result = la_data_client.query_workspace(\r\n",
        "        workspace_id=workspace_id_for_query,\r\n",
        "        query=query,\r\n",
        "        timespan=(start_time, end_time))\r\n",
        "\r\n",
        "    if query_result.status == LogsQueryStatus.SUCCESS:\r\n",
        "        df_la_query = pd.DataFrame(data=query_result.tables[0].rows, columns=query_result.tables[0].columns)\r\n",
        "        print(df_la_query.count())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This approach is likely causing Read timeout and Queue timeout\r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "ip_base = '10.68.88.'\r\n",
        "processes = []\r\n",
        "df_final = pd.DataFrame()\r\n",
        "\r\n",
        "for i in range(1, 10):\r\n",
        "    # create a process with the target function and argument\r\n",
        "    p = multiprocessing.Process(target=query_la_for_multi, args=(df_final, '4d', ip_base + str(i),))\r\n",
        "    # start the process\r\n",
        "    p.start()\r\n",
        "    # add the process to the list\r\n",
        "    processes.append(p)\r\n",
        "\r\n",
        "# wait for all processes to finish\r\n",
        "for p in processes:\r\n",
        "    p.join()\r\n",
        "\r\n",
        "print('done')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joining query"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"let t1 = SecurityAlert | extend ent = parse_json(Entities)| extend ip = tostring(ent[0]['Address']) | project-keep TimeGenerated, ip; let t2 = CommonSecurityLog | where TimeGenerated > ago({0}) | project ip = DestinationIP; t1 | join kind=innerunique t2 on ip\"\r\n",
        "lookback_period = '4d'\r\n",
        "\r\n",
        "query = query.format(lookback_period)\r\n",
        "#print(query)\r\n",
        "df_final = query_la(workspace_id_for_query, query)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_final)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Service Data: MDTI API"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling Microsoft MDTI API for List, the same template can be used for calling other Azure REST APIs with different parameters.\r\n",
        "# For different environments, such as national clouds, you may need to use different root_url, please contact with your admins.\r\n",
        "# It can be ---.azure.us, ---.azure.microsoft.scloud, ---.azure.eaglex.ic.gov, etc.\r\n",
        "def call_mdti_api_for_read(token, resource):\r\n",
        "    \"Calling Microsoft MDTI API\"\r\n",
        "    headers = {\"Authorization\": token, \"content-type\":\"application/json\" }\r\n",
        "    root_url = \"https://graph.microsoft.com\"\r\n",
        "    mdti_url_template = \"{0}/beta/security/threatIntelligence/{1}\"\r\n",
        "    mdti_url = mdti_url_template.format(root_url, resource)\r\n",
        "    # print(mdti_url)\r\n",
        "    response = requests.get(mdti_url, headers=headers, verify=True)\r\n",
        "    return response\r\n",
        "\r\n",
        "def get_token_for_graph():\r\n",
        "    resource_uri = \"https://graph.microsoft.com\"\r\n",
        "    client_id = mssparkutils.credentials.getSecret(akv_name, client_id_name, akv_link_name)\r\n",
        "    client_secret = mssparkutils.credentials.getSecret(akv_name, client_secret_name, akv_link_name)\r\n",
        "\r\n",
        "    credential = ClientSecretCredential(\r\n",
        "        tenant_id=tenant_id, \r\n",
        "        client_id=client_id, \r\n",
        "        client_secret=client_secret)\r\n",
        "    access_token = credential.get_token(resource_uri + \"/.default\")\r\n",
        "    return access_token[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling MDTI API, hosts as example\r\n",
        "header_token_value = \"Bearer {}\".format(get_token_for_graph())\r\n",
        "response_mdti_host = call_mdti_api_for_read(header_token_value, \"hosts('www.microsoft.com')\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_mdti_host \r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data process\r\n",
        "df_host = pd.json_normalize(response_mdti_host.json())\r\n",
        "df_merged = pd.merge(df_la_query, df_host[['id','firstSeenDateTime','registrar']], left_on='Url', right_on='id', how=\"outer\")\r\n",
        "df_final = df_merged.rename(columns = {'TimeGenerated': 'TimeGenerated', 'Url': 'Url', 'registrar': 'Fact'})[['TimeGenerated', 'Url', 'Fact']]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Save result to Azure Log Analytics Custom Table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for data converting\r\n",
        "def convert_dataframe_to_list_of_dictionaries(df, hasTimeGeneratedColumn):\r\n",
        "    list = df.to_dict('records')\r\n",
        "\r\n",
        "    for row in list:\r\n",
        "        # The dataframe may have more than one datetime columns, add all datetiome columns inside this loop, to render ISO 8601\r\n",
        "        if hasTimeGeneratedColumn and str(row['TimeGenerated']) != \"NaT\":\r\n",
        "            row['TimeGenerated']= row['TimeGenerated'].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
        "    \r\n",
        "    return list"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct data body for LA data ingestion\r\n",
        "list_final = convert_dataframe_to_list_of_dictionaries(df_final, True)\r\n",
        "body = list_final"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "body"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data ingestion to LA custom table\r\n",
        "client = LogsIngestionClient(endpoint=dce_endpoint, credential=credential, logging_enable=True)\r\n",
        "\r\n",
        "try:\r\n",
        "    ingestion_result = client.upload(rule_id=immutable_rule_id, stream_name=stream_name, logs=body)\r\n",
        "except HttpResponseError as e:\r\n",
        "    print(f\"Upload failed: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}