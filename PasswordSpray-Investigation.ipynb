{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Password Spray Investigation\n",
    "\n",
    "\n",
    "<table border=\"1 solid black\" align=\"left\">\n",
    "      <tbody>\n",
    "        <tr>\n",
    "          <td>Author</td>\n",
    "          <td>Ian Hellen (@ianhellen, Twitter)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>Date</td>\n",
    "          <td>November 12 2020</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>Version</td>\n",
    "          <td>1.0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>Requirements</td>\n",
    "          <td>msticpy, hvplot</td>\n",
    "        </tr>\n",
    "      </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Picking time periods and events to examine**\n",
    "- Use Time Series analysis to detect unusual numbers of signin failures\n",
    "  - Show anomalies by Signin Result Type\n",
    "  - Analyze anomaly density and select subset of Result Types with strong clustering of failure events\n",
    "  - Calculate time periods for these types\n",
    "  - Allow user to select subset of available periods\n",
    "  - Get data for failures and success logons in these selected periods\n",
    "  \n",
    "**Looking at features of failed signins**\n",
    "1. Analyze Failed signins for repetitive patterns for UserAgent, AppID, DeviceDetails and other Properties\n",
    "  - Pick features/properties based on repetitive nature vs. success signins\n",
    "  - Try to find any successful logons that show the same values\n",
    "2. Client IP Origin\n",
    "  - Look at netblocks for client IPs - do failed logons come from same IP blocks as sucessful logins?\n",
    "  - Highlight all failures coming from netblocks unused or rarely used in success logins.\n",
    "3. Logon Request Timing\n",
    "  - Do the failed logon events occur at specific min/second past hour - indicating an automated process initiating them.\n",
    "  - Identify failures that have a regular timed pattern\n",
    "4. IP Location data\n",
    "  - Similar to netblocks - establish common locations for successful logons\n",
    "  - Highlight all failed requests coming from unused or rare location\n",
    "5. User Agent\n",
    "  - Identify User Agents present in success logins\n",
    "  - Identify all failed requests using UA that is not in this list\n",
    "6. User Agent - AppId mapping\n",
    "  - Look at frequency of pairing between User Agents and AppIds in failed and successful signins\n",
    "7. Legacy authentication protocols\n",
    "  - How often are legacy authentication protocols used in success vs. failed logins?\n",
    "  - Are specific patterns in failed logons associated with a specific protocol\n",
    "8. Pattern analysis in UA strings\n",
    "  - This is not yet done.\n",
    "9. IP Addresses with multiple names\n",
    "  - It would be ususual for logons coming from a single IP address to be using different user names\n",
    "  - Any that do this are flagged as suspicious\n",
    "10. User names with multiple IP Addresses, UserAgents, Locations\n",
    "  - While it is not unusual for users to be associated with multiple of these, seeing many of these may be suspicious\n",
    "  - Failed logons with multiple values may indicated that the user name is \n",
    "    public knowlege and being attacked from multiple sources\n",
    "11. IP Addresses identified as malicious by Azure Active Directory.\n",
    "  - This information is usually only available to customers on a certain paid tier of AAD so we try not to depend on this.\n",
    "  - If the information is available though, let's use it.\n",
    "\n",
    "**Threat intelligence lookups for suspect IP addresses**\n",
    "- We've collected suspicious IP addresses in many of the previous steps. We do threat intel\n",
    "  lookups on those IPs to check for known bad origins\n",
    "\n",
    "**Viewing the investigation results**\n",
    "- Use above properties to cluster failed events based on features.\n",
    "  - You can browse the sets clustered events - allowing large numbers of failure events\n",
    "    to be consumed in manageable chunks<br>\n",
    "- Observation browsing\n",
    "  - The results from the feature analysis are collected together in a browseable list\n",
    "    so that you don't have to trawl through the entired notebook to see the interesting data.<br>\n",
    "- Results query\n",
    "  - This is a simple interactive query interface allowing you to query subsets of rows\n",
    "    and columns of the output data. You can also export the results of the query\n",
    "    to the clipboard for analysis in other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Notebook\n",
    "The notebook contains a lot of details and illustrations of the analysis but most of\n",
    "that is peripheral to actually checking your data for possible password spray events.\n",
    "This additional analysis and visualization is left in the notebook for better explanation\n",
    "of the techniques used - and make them easier to change/correct, if needed.\n",
    "\n",
    "### Prerequisites\n",
    "- msticpy - install the latest using `pip install --upgrade msticpy`\n",
    "- hvplot - this will be installed by the `init_notebook` function in the following cell.\n",
    "\n",
    "### Running the notebook\n",
    "The quick and easy way of using the notebook is as follows:\n",
    "- Individually run all of the cells up to the start of Part 1:\n",
    "  - library initialization and installation\n",
    "  - authenticating to the workspace\n",
    "  - setting notebook parameters\n",
    "  - setting the time boundaries for the analysis\n",
    "- From the beginning of Part 1, select `Run selected cell and all below` from the notebook `Run` menu.\n",
    "- Wait a while...\n",
    "- Navigate to the summary browsers at the end of the notebook:\n",
    "  - [Browse clustered events](#4.2-Browse-clustered-events)\n",
    "  - [Browse investigation observations](#4.3-Browse-investigation-observations)\n",
    "  - [Results interactive query](#4.4-Results-query-browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:49:01.768356Z",
     "start_time": "2020-08-24T01:48:53.765357Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "REQ_PYTHON_VER=(3, 6)\n",
    "REQ_MSTICPY_VER=(0, 6, 0)\n",
    "\n",
    "display(HTML(\"<h3>Starting Notebook setup...</h3>\"))\n",
    "if Path(\"./utils/nb_check.py\").is_file():\n",
    "    from utils.nb_check import check_mp_ver, check_python_ver\n",
    "\n",
    "    check_python_ver(min_py_ver=REQ_PYTHON_VER)\n",
    "    try:\n",
    "        check_mp_ver(min_msticpy_ver=REQ_MSTICPY_VER)\n",
    "    except ImportError:\n",
    "        !pip install --upgrade msticpy\n",
    "        if \"msticpy\" in sys.modules:\n",
    "            importlib.reload(sys.modules[\"msticpy\"])\n",
    "        else:\n",
    "            import msticpy\n",
    "        check_mp_ver(REQ_MSTICPY_VER)\n",
    "\n",
    "# If you are not launching this notebook from Azure Sentinel, install msticpy with\n",
    "# !pip install msticpy\n",
    "\n",
    "from msticpy.nbtools import nbinit\n",
    "\n",
    "nbinit.init_notebook(\n",
    "    namespace=globals(),\n",
    "    extra_imports=[\n",
    "        \"ipwhois, IPWhois\",\n",
    "        \"statistics, variance\"\n",
    "    ],\n",
    "    additional_packages = [\"hvplot\"],\n",
    "    friendly_exceptions=False,\n",
    ")\n",
    "\n",
    "import hvplot.pandas\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import reset_output, show\n",
    "\n",
    "# pd.set_option(\"plotting.backend\", \"pandas_bokeh\")\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data provider and Authenticate\n",
    "ws_config = WorkspaceConfig(workspace=YOUR_WORKSPACE_NAME)\n",
    "qry_prov = QueryProvider(data_environment=\"LogAnalytics\")\n",
    "qry_prov.connect(connection_str=ws_config.code_connect_str)\n",
    "table_index = qry_prov.schema_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def use_caching() -> bool:\n",
    "    \"\"\"True if using caching for reference data.\"\"\"\n",
    "    return use_cache_cb.value\n",
    "\n",
    "\n",
    "def mask_data() -> bool:\n",
    "    \"\"\"True if using data masking\"\"\"\n",
    "    return mask_data_cb.value\n",
    "\n",
    "\n",
    "def cache_folder() -> str:\n",
    "    return cache_folder_txt.value\n",
    "\n",
    "\n",
    "cwd = str(Path(\".\").resolve())\n",
    "use_cache_cb = widgets.Checkbox(\n",
    "    description=\"Use cached data\", value=True, **WIDGET_DEFAULTS\n",
    ")\n",
    "cache_folder_txt = widgets.Text(\n",
    "    description=\"Cache data folder\", value=cwd, **WIDGET_DEFAULTS\n",
    ")\n",
    "mask_data_cb = widgets.Checkbox(\n",
    "    description=\"Mask PII data\", value=False, **WIDGET_DEFAULTS\n",
    ")\n",
    "\n",
    "md(\"Notebook setup options<hr>\", \"large, bold\")\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        [use_cache_cb, cache_folder_txt, mask_data_cb], style={\"border-style\": \"solid\"}\n",
    "    )\n",
    ")\n",
    "md(\"<hr>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set time boundaries for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:49:13.199682Z",
     "start_time": "2020-08-24T01:49:13.140686Z"
    }
   },
   "outputs": [],
   "source": [
    "query_time = nbwidgets.QueryTime(\n",
    "    units=\"day\", max_before=60, max_after=30, before=30, after=0, auto_display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1 - Detecting anomalous logon events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 Using Time Series to detect anomalous patterns in login failures\n",
    "\n",
    "### Query Signing Logs for TimeSeries Anomalies for failure logons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_ts_df = qry_prov.MultiDataSource.get_timeseries_anomalies(\n",
    "    # \"print\",\n",
    "    table=\"SigninLogs\",\n",
    "    start=query_time.start,\n",
    "    end=query_time.end,\n",
    "    timestampcolumn=\"TimeGenerated\",\n",
    "    aggregatecolumn=\"AppDisplayName\",\n",
    "    groupbycolumn=\"ResultType\",\n",
    "    aggregatefunction=\"count(AppDisplayName)\",\n",
    "    where_clause=\"| where ResultType != 0\",\n",
    "    add_query_items=\"| project-rename Total=AppDisplayName\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: AAD Signin Log Type Code descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:49:18.639194Z",
     "start_time": "2020-08-24T01:49:18.627196Z"
    }
   },
   "outputs": [],
   "source": [
    "result_types = {\n",
    "    \"0\": \"Success\",\n",
    "    \"50053\": \"Account is locked because user tried to sign in too many times with an incorrect user ID or password.\",\n",
    "    \"50055\": \"Invalid password, entered expired password.\",\n",
    "    \"50057\": \"User account is disabled. The account has been disabled by an administrator.\",\n",
    "    \"50072\": \"Users' needs to enroll for second factor authentication (interactive).\",\n",
    "    \"50074\": \"User did not pass the MFA challenge.\",\n",
    "    \"50076\": \"User did not pass the MFA challenge (non interactive).\",\n",
    "    \"50079\": \"User needs to enroll for second factor authentication.\",\n",
    "    \"50097\": \"Device Authentication Required - DeviceId -DeviceAltSecId claims are null OR no device corresponding to the device identifier exists.\",\n",
    "    \"50105\": \"The signed in user is not assigned to a role for the signed in application. Assign the user to the application. For more information: https://docs.microsoft.com/en-us/azure/active-directory/application-sign-in-problem-federated-sso-gallery#user-not-assigned-a-role.\",\n",
    "    \"50126\": \"Invalid username or password or Invalid on-premise username or password.\",\n",
    "    \"50155\": \"Device authentication failed for this user.\",\n",
    "    \"53000\": \"Conditional Access policy requires a compliant device, and the device is not compliant. Have the user enroll their device with an approved MDM provider like Intune.\",\n",
    "    \"53003\": \"Access has been blocked due to conditional access policies.\",\n",
    "}\n",
    "\n",
    "result_type_categories = {\n",
    "    \"50053\": \"invalid_credentials\",\n",
    "    \"50055\": \"invalid_credentials\",\n",
    "    \"50057\": \"access_policy\",\n",
    "    \"50072\": \"mfa\",\n",
    "    \"50074\": \"mfa\",\n",
    "    \"50076\": \"mfa\",\n",
    "    \"50079\": \"mfa\",\n",
    "    \"50097\": \"mfa\",\n",
    "    \"50105\": \"access_policy\",\n",
    "    \"50126\": \"invalid_credentials\",\n",
    "    \"50155\": \"mfa\",\n",
    "    \"53000\": \"access_policy\",\n",
    "    \"53003\": \"access_policy\",\n",
    "}\n",
    "\n",
    "client_apps = {\n",
    "    \"Browser\": \"modern\",\n",
    "    \"Mobile Apps and Desktop clients\": \"modern\",\n",
    "    \"Authenticated SMTP\": \"legacy\",\n",
    "    \"AutoDiscover\": \"legacy\",\n",
    "    \"Exchange ActiveSync\": \"legacy\",\n",
    "    \"Exchange Online PowerShell\": \"legacy\",\n",
    "    \"Exchange Web Services\": \"legacy\",\n",
    "    \"IMAP4\": \"legacy\",\n",
    "    \"MAPI Over HTTP\": \"legacy\",\n",
    "    \"Offline Address Book\": \"legacy\",\n",
    "    \"Other clients\": \"legacy\",\n",
    "    \"Outlook Anywhere (RPC over HTTP)\": \"legacy\",\n",
    "    \"POP3\": \"legacy\",\n",
    "    \"Reporting Web Services\": \"legacy\",\n",
    "    \"Universal Outlook\": \"legacy\",\n",
    "}\n",
    "\n",
    "\n",
    "def legacy_clients() -> list:\n",
    "    \"\"\"Return a list of legacy client protocols.\"\"\"\n",
    "    return [app for app, cat in client_apps.items() if cat == \"legacy\"]\n",
    "\n",
    "\n",
    "def is_legacy_client(client_app: str) -> bool:\n",
    "    \"\"\"Return True if client_app is legacy.\"\"\"\n",
    "    return client_apps.get(client_app) == \"legacy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing Anomaly Graphs for all Result Types\n",
    "\n",
    "Here we are showing time series anomaly charts for each Result Type.\n",
    "\n",
    "Following this we will choose particular subsets of event types and time ranges to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_anom_plots = nbwidgets.OptionButtons(\n",
    "    description=\"Show anomaly plots for all types?\",\n",
    "    buttons=[\"Yes\", \"No\"],\n",
    "    default=\"No\",\n",
    "    timeout=5,\n",
    ")\n",
    "await show_anom_plots.display_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:50:03.854164Z",
     "start_time": "2020-08-24T01:50:03.692168Z"
    }
   },
   "outputs": [],
   "source": [
    "from msticpy.nbtools.timeseries import display_timeseries_anomolies\n",
    "\n",
    "if show_anom_plots.value == \"yes\":\n",
    "    plot_res_types = [res_id for res_id in result_types if res_id != \"0\"]\n",
    "\n",
    "    for event_type in plot_res_types:\n",
    "        ts_data = signin_ts_df[signin_ts_df[\"ResultType\"] == event_type]\n",
    "        if not ts_data.empty:\n",
    "            plot = display_timeseries_anomolies(\n",
    "                data=ts_data,\n",
    "                title=f\"Timeline anomalies for event type {event_type}: {result_types[event_type]}\",\n",
    "            )\n",
    "        else:\n",
    "            md(f\"No events matching selected type {event_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Narrow the data down to specific events and time ranges\n",
    "\n",
    "If we combine all of the anomaly data on to a single timeline, you can \n",
    "see that there are periods with **higher density of events**. \n",
    "However, it is difficult to pick out these periods accurately.\n",
    "\n",
    "Also, some Event/Result types have relatively few anomalies and may\n",
    "not be interesting to look at further.\n",
    "\n",
    "We're going to do some initial analysis to pick out potentally more\n",
    "interesting ResultTypes and time ranges to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_ts_df[\n",
    "    (signin_ts_df[\"ResultType\"].isin([str(e_type) for e_type in plot_res_types]))\n",
    "    & (signin_ts_df[\"anomalies\"] == 1)\n",
    "].mp_timeline.plot(source_columns=[\"ResultType\"], group_by=\"ResultType\", height=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Anomaly density: which event types had high clustering with respect to time?\n",
    "\n",
    "We want to get a number that gives us an indication of which event types have concentrated\n",
    "bursts of anomalies, as opposed to a lot of anomalies that are distributed across the time range.\n",
    "\n",
    "The intuition here is that password spray attacks are likely to be concentrated over a relatively\n",
    "short period.\n",
    "\n",
    "We calculate approximate **anomaly density** for each *Result Type* by:\n",
    "- taking the time differences (in seconds) between individual anomaly instances for a give *ResultType*.\n",
    "- then take the median value of these time differences differences.\n",
    "\n",
    "The **density** for a given *ResultType* is calculated by dividing the total number anomalies by the median of time differences.\n",
    "\n",
    "For N anomalies - a series with lots of shorter time intervals will have a higher\n",
    "density score than a series that is more evenly distributed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_scoring = []\n",
    "for event_type in plot_res_types:\n",
    "    event_type_score = {}\n",
    "    ts_data = signin_ts_df[signin_ts_df[\"ResultType\"] == event_type]\n",
    "    anomalies = ts_data[ts_data.anomalies == 1]\n",
    "    total_anomalies = len(anomalies)\n",
    "    total_events = anomalies.Total.sum()\n",
    "    if total_anomalies > 1:\n",
    "        # Calculate median time difference between anomalies\n",
    "        median_diff = anomalies.TimeGenerated.diff().median()\n",
    "        median_sec_diff = median_diff.total_seconds()\n",
    "    else:\n",
    "        # If there is only one anomaly, set time difference of 24 hours\n",
    "        median_sec_diff = 24 * 3600\n",
    "    score = np.sum(np.abs(anomalies.score))\n",
    "\n",
    "    event_type_score[\"event_type\"] = event_type\n",
    "    event_type_score[\"anom_count\"] = total_events\n",
    "    event_type_score[\"anom perc\"] = total_events / len(ts_data)\n",
    "    event_type_score[\"anom_total_score\"] = score\n",
    "    event_type_score[\"median_sec_diff\"] = median_sec_diff\n",
    "    event_type_score[\"anomaly_density\"] = total_events / median_sec_diff\n",
    "    event_type_scoring.append(event_type_score)\n",
    "\n",
    "event_score_df = pd.DataFrame(event_type_scoring).sort_values(\n",
    "    \"anomaly_density\", ascending=False\n",
    ")\n",
    "display(\n",
    "    event_score_df.hvplot.bar(\n",
    "        x=\"event_type\",\n",
    "        y=\"anomaly_density\",\n",
    "        title=\"Relative anomaly density by ResultType\",\n",
    "    )\n",
    ")\n",
    "display(event_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Event Types to be used\n",
    "\n",
    "We use the density calculation to select a set of events with the highest density.\n",
    "\n",
    "#### Default selection is any *ResultType* with a density > 5% of the max *ResultType* density\n",
    "We use this threshold to auto-select *ResultType*s in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_density = widgets.FloatSlider(\n",
    "    min=event_score_df.anomaly_density.min(),\n",
    "    max=event_score_df.anomaly_density.max(),\n",
    "    value=event_score_df.anomaly_density.max() * 0.05,\n",
    "    step=0.0001,\n",
    "    description=\"Anomaly density lower limit\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    "    style={\"description_width\": \"200px\"},\n",
    "    readout_format=\".7f\",\n",
    ")\n",
    "display(anom_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_event_types(fmt=\"list\"):\n",
    "    \"\"\"\n",
    "    Return list of currently selected events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fmt: str\n",
    "        Possible values are:\n",
    "        - list == list of string values\n",
    "        - int_list == list of ints\n",
    "        - str == comma-separated values as string\n",
    "        - q_str == like 'str' but values are quoted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list or str\n",
    "        depending on `fmt` parameter\n",
    "    \"\"\"\n",
    "    sel_events = wgt_event_type.value\n",
    "    if fmt == \"list\":\n",
    "        return [str(evt_id) for evt_id in sel_events]\n",
    "    if fmt == \"str\":\n",
    "        return \", \".join([str(evt_id) for evt_id in sel_events])\n",
    "    if fmt == \"int_list\":\n",
    "        return list(sel_events)\n",
    "    if fmt == \"q_str\":\n",
    "        return \", \".join([f\"'{evt_id}'\" for evt_id in sel_events])\n",
    "\n",
    "\n",
    "def selected_events_label():\n",
    "    \"\"\"Return a string of selected event types for labelling saved data.\"\"\"\n",
    "    \"_\".join(selected_event_types())\n",
    "\n",
    "\n",
    "# Create widgets\n",
    "layout = widgets.Layout(width=\"50%\", height=\"200px\")\n",
    "style = {\"description_width\": \"200px\"}\n",
    "\n",
    "wgt_event_type = widgets.SelectMultiple(\n",
    "    description=\"Select Result types to use\",\n",
    "    options=[\n",
    "        (f\"{evt_id}: ({result_type_categories.get(evt_id)}) {evt_desc}\", evt_id)\n",
    "        for evt_id, evt_desc in result_types.items()\n",
    "        if evt_id != \"0\"\n",
    "    ],\n",
    "    layout=layout,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "calc_types = list(\n",
    "    event_score_df[event_score_df.anomaly_density > anom_density.value].event_type\n",
    ")\n",
    "\n",
    "categories = [\"all\", \"calculated - based on anomaly density\"] + sorted(\n",
    "    list(set(result_type_categories.values()))\n",
    ")\n",
    "\n",
    "wgt_event_cat = widgets.RadioButtons(\n",
    "    description=\"Select Result Types of category\",\n",
    "    options=categories,\n",
    "    value=\"invalid_credentials\",\n",
    "    layout=layout,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "\n",
    "def _set_by_category(change):\n",
    "    sel_cat = change.get(\"new\")\n",
    "    if sel_cat == \"all\":\n",
    "        wgt_event_type.value = [opt[1] for opt in wgt_event_type.options]\n",
    "    elif sel_cat == \"calculated\":\n",
    "        wgt_event_type.value = calc_types\n",
    "    else:\n",
    "        wgt_event_type.value = [\n",
    "            evt_id for evt_id, cat in result_type_categories.items() if cat == sel_cat\n",
    "        ]\n",
    "\n",
    "\n",
    "_set_by_category({\"new\": \"invalid_credentials\"})\n",
    "wgt_event_cat.observe(_set_by_category, names=\"value\")\n",
    "\n",
    "\n",
    "md(\n",
    "    \"\"\"\n",
    "<p style='background-color: beige; font-size: medium'>\n",
    "<b>Tip</b> - Performing analysis with multiple result categories can create confusing results.<br>\n",
    "We recommend you select one or two Result Types at a time and\n",
    "run through the notebook with these subsets.<br>\n",
    "Alternatively, use the category selector and pick a category. Start with \"invalid_credentials\".\n",
    "\n",
    "</p>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "display(widgets.HBox([wgt_event_type, wgt_event_cat]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Combined Anomaly Data Time Series for these *ResultType*s\n",
    "Select the event type(s) to view above and run the following cell.\n",
    "\n",
    "> Warning: selecting multiple types make produce unclear trend an anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:50:03.854164Z",
     "start_time": "2020-08-24T01:50:03.692168Z"
    }
   },
   "outputs": [],
   "source": [
    "from msticpy.nbtools.timeseries import display_timeseries_anomolies\n",
    "\n",
    "event_types = selected_event_types()\n",
    "ts_data = signin_ts_df[signin_ts_df[\"ResultType\"].isin(event_types)]\n",
    "if not ts_data.empty:\n",
    "    if len(event_types) <= 2:\n",
    "        title = [\n",
    "            f\"{eid} - {desc}\"\n",
    "            for eid, desc in result_types.items()\n",
    "            if str(eid) in event_types\n",
    "        ]\n",
    "    else:\n",
    "        title = event_types\n",
    "    plot = display_timeseries_anomolies(\n",
    "        data=ts_data, title=f\"Timeline anomalies for event types {title}\", height=800\n",
    "    )\n",
    "else:\n",
    "    md(\"No events matching selected types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract time-intervals around the anomalies\n",
    "Next we want to narrow down the time ranges to the periods of greatest anomaly activity.\n",
    "\n",
    "> Note: the selection of events is based on the last graph.<br>\n",
    "\n",
    "First, we will extract time periods around each positive anomaly and calculate a 1 hour\n",
    "time window around it (merging successive anomaly points into single time regions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T20:30:00.900005Z",
     "start_time": "2020-08-24T20:30:00.751006Z"
    }
   },
   "outputs": [],
   "source": [
    "from msticpy.analysis.timeseries import (\n",
    "    create_time_period_kqlfilter,\n",
    "    extract_anomaly_periods,\n",
    ")\n",
    "\n",
    "anom_periods = extract_anomaly_periods(\n",
    "    signin_ts_df[signin_ts_df[\"ResultType\"].isin(selected_event_types())], period=\"1H\"\n",
    ")\n",
    "anom_per_df = pd.DataFrame(data=anom_periods.items(), columns=[\"start\", \"end\"])\n",
    "md(f\"Number of anomalous periods extracted = {len(anom_per_df)}\")\n",
    "\n",
    "all_periods_crit = (signin_ts_df[\"ResultType\"].isin(selected_event_types())) & (\n",
    "    signin_ts_df[\"anomalies\"] == 1\n",
    ")\n",
    "for idx, row in anom_per_df.iterrows():\n",
    "    per_crit = (signin_ts_df[\"TimeGenerated\"] >= row.start) & (\n",
    "        signin_ts_df[\"TimeGenerated\"] <= row.end\n",
    "    )\n",
    "    anom_count = signin_ts_df[all_periods_crit & per_crit].anomalies.count()\n",
    "    anom_per_df.loc[idx, \"anom_count\"] = anom_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show extracted periods graphically\n",
    "#### (just a bit easier than eyeballing a list)\n",
    "\n",
    "Usually the graph below will show a few large blocks and a lot of smaller periods.\n",
    "\n",
    "The line thickness is proportional to the number of anomalies in the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T20:27:51.508630Z",
     "start_time": "2020-08-24T20:27:51.338629Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show\n",
    "from bokeh.models import Arrow, ColumnDataSource, OpenHead\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "avail_periods_plot = figure(\n",
    "    plot_height=300,\n",
    "    plot_width=900,\n",
    "    x_axis_label=\"Time\",\n",
    "    x_axis_type=\"datetime\",\n",
    "    x_minor_ticks=10,\n",
    "    tools=[\"xwheel_zoom\", \"box_zoom\", \"reset\", \"save\", \"xpan\"],\n",
    "    toolbar_location=\"above\",\n",
    "    title=\"Available time ranges (line thickness == number of anomalies in period)\",\n",
    ")\n",
    "\n",
    "avail_periods_plot.circle(size=4, x=\"start\", y=1, color=\"red\", source=anom_per_df)\n",
    "avail_periods_plot.circle(size=4, x=\"end\", y=1, color=\"red\", source=anom_per_df)\n",
    "for idx, row in anom_per_df.iterrows():\n",
    "    anoms = row[\"anom_count\"]\n",
    "    avail_periods_plot.add_layout(\n",
    "        Arrow(\n",
    "            end=OpenHead(line_color=\"green\", line_width=1, size=5),\n",
    "            start=OpenHead(line_color=\"green\", line_width=1, size=5),\n",
    "            x_start=row[\"start\"],\n",
    "            y_start=1,\n",
    "            x_end=row[\"end\"],\n",
    "            y_end=1,\n",
    "            line_width=anoms,\n",
    "        )\n",
    "    )\n",
    "show(avail_periods_plot)\n",
    "md(\"Zoom in to see period details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of periods to use\n",
    "We don't want to use all of these anomaly periods - just the ones with most activity\n",
    "\n",
    "#### Calculate Number of anomalies per period and trim periods that have low numbers of anomalies\n",
    "We do this to avoid having to query a lot of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_thresh = widgets.IntSlider(\n",
    "    min=1,\n",
    "    value=10,\n",
    "    max=anom_per_df.anom_count.max(),\n",
    "    description=\"Anomaly threshold\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    "    style={\"description_width\": \"150px\"},\n",
    ")\n",
    "md(\"Select lower bound for number of anomalies in period.\", \"bold\")\n",
    "display(anom_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the results of applying this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_opts = {\n",
    "    f\"{data.start} - {data.end} ({data.end - data.start})  anomalies = {data.anom_count}\": idx\n",
    "    for idx, data in anom_per_df.iterrows()\n",
    "}\n",
    "selected = list(anom_per_df[anom_per_df[\"anom_count\"] >= anom_thresh.value].index)\n",
    "select_periods = widgets.SelectMultiple(\n",
    "    options=all_opts,\n",
    "    #     value=selected,\n",
    "    description=\"Pick periods to examine\",\n",
    "    layout=widgets.Layout(width=\"50%\", height=\"300px\"),\n",
    ")\n",
    "select_periods.value = selected\n",
    "display(select_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot updated selection ranges to see what we ended up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T20:27:51.508630Z",
     "start_time": "2020-08-24T20:27:51.338629Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show\n",
    "from bokeh.models import Arrow, ColumnDataSource, OpenHead\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "select_periods_plot = figure(\n",
    "    plot_height=300,\n",
    "    plot_width=900,\n",
    "    x_axis_label=\"Time\",\n",
    "    x_axis_type=\"datetime\",\n",
    "    x_minor_ticks=10,\n",
    "    tools=[\"xwheel_zoom\", \"box_zoom\", \"reset\", \"save\", \"xpan\"],\n",
    "    toolbar_location=\"above\",\n",
    "    title=\"Selected time ranges (thickness == # anomalies)\",\n",
    ")\n",
    "sel_crit = anom_per_df.index.isin(select_periods.value)\n",
    "\n",
    "select_periods_plot.circle(\n",
    "    size=4, x=\"start\", y=1, color=\"red\", source=anom_per_df[sel_crit]\n",
    ")\n",
    "select_periods_plot.circle(\n",
    "    size=4, x=\"end\", y=1, color=\"red\", source=anom_per_df[sel_crit]\n",
    ")\n",
    "for idx, row in anom_per_df[sel_crit].iterrows():\n",
    "    anoms = row[\"anom_count\"]\n",
    "    select_periods_plot.add_layout(\n",
    "        Arrow(\n",
    "            end=OpenHead(line_color=\"green\", line_width=1, size=5),\n",
    "            start=OpenHead(line_color=\"green\", line_width=1, size=5),\n",
    "            x_start=row[\"start\"],\n",
    "            y_start=1,\n",
    "            x_end=row[\"end\"],\n",
    "            y_end=1,\n",
    "            line_width=anoms,\n",
    "        )\n",
    "    )\n",
    "show(select_periods_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1.3 Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 - Caching reference data\n",
    "To reduce the number of expensive queries we use a caching system.\n",
    "The notebook **always** queries fresh *failed logon* data. Failure logons\n",
    "are not affected by this. \n",
    "\n",
    "However, in several places we also query reference *success logon* data that\n",
    "we use for comparison - as baselines indicating normal behavior. \n",
    "By default, we will cache this reference data and re-use this\n",
    "data as long as the date range you are querying for matches the cached data.\n",
    "\n",
    "> **Note** this has the unfortunate side effect of littering your directory\n",
    "> with pickled DataFrames but it should be easy to clean these up when\n",
    "> you no longer need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Caching functions\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def archive_name(base_name, start, end):\n",
    "    \"\"\"Return file name formatted with dates.\"\"\"\n",
    "    st_fmt = re.sub(r\"[:\\.+]\", \"_\", str(start))\n",
    "    end_fmt = re.sub(r\"[:\\.+]\", \"_\", str(end))\n",
    "    return f\"{base_name}-{st_fmt}-{end_fmt}.pkl\"\n",
    "\n",
    "\n",
    "def archive_exists(f_name, folder=None):\n",
    "    \"\"\"Return True if the filename exists.\"\"\"\n",
    "    folder = folder or cache_folder()\n",
    "    return Path(folder).joinpath(f_name).is_file()\n",
    "\n",
    "\n",
    "def get_archive(f_name, folder=None):\n",
    "    \"\"\"Return the unpickled DataFrame.\"\"\"\n",
    "    folder = folder or cache_folder()\n",
    "    if use_caching() and archive_exists(f_name, folder):\n",
    "        return pd.read_pickle(str(Path(folder).joinpath(f_name)))\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_archive(f_name, data, folder=None):\n",
    "    \"\"\"Save (pickle) data to the named file.\"\"\"\n",
    "    if use_caching():\n",
    "        folder = folder or cache_folder()\n",
    "        data.to_pickle(str(Path(folder).joinpath(f_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 - Data masking\n",
    "We import and use a few functions that mask data. You might need to do this\n",
    "if you are sharing the notebook outside your organization.\n",
    "\n",
    "The are controlled by the **Mask data** checkbox at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msticpy.data.data_obfus import hash_ip, hash_item, replace_guid\n",
    "\n",
    "\n",
    "def mask_names(series):\n",
    "    return series.apply(lambda x: hash_item(x, \"@.\")) if mask_data() else series\n",
    "\n",
    "\n",
    "def mask_guids(series):\n",
    "    return series.apply(lambda x: replace_guid(x)) if mask_data() else series\n",
    "\n",
    "\n",
    "def mask_ips(series):\n",
    "    return series.apply(lambda x: hash_ip(x)) if mask_data() else series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 - Observation Tracking\n",
    "We'll use this to track findings as we go through the notebook. These are collated into a browseable form at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from msticpy.nbtools import Observations\n",
    "obs_log = Observations()\n",
    "\n",
    "IPEntry = namedtuple(\"IPEntry\", \"address, reason, section\")\n",
    "ip_log = []\n",
    "\n",
    "\n",
    "def fmt_link(title):\n",
    "    return title.replace(\" \", \"-\")\n",
    "\n",
    "\n",
    "def create_ip_entries(ip_addresses, reason, section):\n",
    "    return [IPEntry(address=ip, reason=reason, section=section) for ip in ip_addresses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Use the selected time periods to get *all* logon failures for those periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch all failed and success events and save to 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "obs_log.add_observation(caption=\"Anomaly periods\", data=anom_per_df[sel_crit])\n",
    "\n",
    "periods_dict = {row.start: row.end for row in anom_per_df[sel_crit].itertuples()}\n",
    "time_filter = create_time_period_kqlfilter(periods_dict)\n",
    "\n",
    "field_list = [\n",
    "    \"TimeGenerated\",\n",
    "    \"DeviceDetail\",\n",
    "    \"ResultType\",\n",
    "    \"ResultDescription\",\n",
    "    \"IPAddress\",\n",
    "    \"Location\",\n",
    "    \"LocationDetails\",\n",
    "    \"AppId\",\n",
    "    \"AppDisplayName\",\n",
    "    \"UserAgent\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserId\",\n",
    "    \"ResourceDisplayName\",\n",
    "    \"ClientAppUsed\",\n",
    "]\n",
    "\n",
    "project_fields = \", \".join(field_list)\n",
    "\n",
    "qry = \"\"\"\n",
    "SigninLogs\n",
    "| where TimeGenerated between (datetime({start}) .. datetime({end}))\n",
    "| project {field_list}\n",
    "| where ResultType {result_filter}\n",
    "| extend device_str = tostring(DeviceDetail)\n",
    "\"\"\"\n",
    "\n",
    "md(\"We split the query into chunks to avoid timeouts\")\n",
    "print(\"Getting logon failures...\")\n",
    "failed_results = []\n",
    "for period in tqdm(\n",
    "    anom_per_df[sel_crit].itertuples(), desc=\"Queries\", total=len(anom_per_df[sel_crit])\n",
    "):\n",
    "    failed_results.append(\n",
    "        qry_prov.exec_query(\n",
    "            qry.format(\n",
    "                start=period.start,\n",
    "                end=period.end,\n",
    "                field_list=project_fields,\n",
    "                result_filter=f\"in ({selected_event_types(fmt='str')})\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "failed_signin_list_df = pd.concat(failed_results, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Getting sample logon success...\")\n",
    "\n",
    "success_results = []\n",
    "for idx, period in tqdm(\n",
    "    enumerate(anom_per_df[sel_crit].itertuples()),\n",
    "    desc=\"Queries\",\n",
    "    total=len(anom_per_df[sel_crit]),\n",
    "):\n",
    "    file_name = archive_name(\"success_signin_list_df\", period.start, period.end)\n",
    "\n",
    "    # check if saved data files are present. If so we can use these.\n",
    "    if not use_caching() or not archive_exists(file_name):\n",
    "        num_failed_for_time_period = len(failed_results[idx])\n",
    "        sample_size = max(num_failed_for_time_period, 5000)\n",
    "        success_df = qry_prov.exec_query(\n",
    "            qry.format(\n",
    "                start=period.start,\n",
    "                end=period.end,\n",
    "                field_list=project_fields,\n",
    "                result_filter=f\"== 0 | sample {sample_size}\",\n",
    "            )\n",
    "        )\n",
    "        save_archive(file_name, data=success_df)  # nop if not using caching\n",
    "    else:\n",
    "        success_df = get_archive(file_name)\n",
    "        print(\"Using cached \", file_name)\n",
    "    success_results.append(success_df)\n",
    "\n",
    "success_signin_list_df = pd.concat(success_results, ignore_index=True)\n",
    "del failed_results, success_results\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2 - Finding patterns in failed logons\n",
    "\n",
    "---\n",
    "\n",
    "One of the characteristic that we are looking for is repetitive behavior in our failed logons.\n",
    "Since we can assume that password spray attacks (because of their bulk nature) are carried\n",
    "out programmatically, we might be able to tell automated logon attempts from legitimate\n",
    "attempts.\n",
    "\n",
    "In legitimate logon attempts, we would expect some variability due to things like:\n",
    "- Authenticating to different services/applications\n",
    "- Timing of the attempts\n",
    "- Differences in the IP patterns for the Client IP\n",
    "\n",
    "We would also expect the logons to originate from a fairly stable set of locations.\n",
    "\n",
    "Finally, we can check characteristics of the client device used and see if it matches any used by legitimate successful logons.\n",
    "\n",
    "Features\n",
    "- 2.1 Properties with low variability\n",
    "- 2.2 IP Block usage\n",
    "- 2.3 Login Request timing\n",
    "- 2.4 IP Location\n",
    "- 2.5 User Agent strings\n",
    "- 2.6 IP Addresses with multiple users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Looking for Properties that have a low variability in Failed vs. Success Logons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the *ResultType* subset to use in this section\n",
    "\n",
    "Multiple selections are possible but you may find the results are clearer in this section if you work on one failure type at a time.\n",
    "\n",
    "Select the Failure type(s) and run the rest of the cells in this section. Then repeat for different types.\n",
    "\n",
    "> Note This selection only affects the subsequent queries in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    f\"{res[0]}: {res[1]}\": res[0]\n",
    "    for res in failed_signin_list_df[[\"ResultType\", \"ResultDescription\"]]\n",
    "    .drop_duplicates()\n",
    "    .values\n",
    "}\n",
    "select_res_type = widgets.SelectMultiple(\n",
    "    options=opts,\n",
    "    description=\"Select failure type:\",\n",
    "    layout=widgets.Layout(width=\"70%\", height=\"150px\"),\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "select_res_type.value = list(opts.values())\n",
    "select_res_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display subset (top 30)\n",
    "Typically, you will see logon patterns with high numbers of failures near the top of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_cols = [\"ResourceDisplayName\", \"AppId\", \"ResultType\", \"device_str\", \"UserAgent\"]\n",
    "fail_res_type_filter = failed_signin_list_df[\"ResultType\"].isin(select_res_type.value)\n",
    "failed_subset = (\n",
    "    failed_signin_list_df[[*disp_cols, \"TimeGenerated\"]][fail_res_type_filter]\n",
    "    .groupby(disp_cols)\n",
    "    .count()\n",
    "    .rename(columns={\"TimeGenerated\": \"Count\"})\n",
    "    .query(\"Count > 5\")\n",
    "    .sort_values(\"Count\", ascending=False)\n",
    ")\n",
    "\n",
    "display(failed_subset.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the top N rows to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_value = len(failed_subset) // 5 if len(failed_subset) > 30 else len(failed_subset)\n",
    "rows_select = widgets.IntSlider(\n",
    "    min=1,\n",
    "    max=len(failed_subset),\n",
    "    value=def_value,\n",
    "    description=\"Top N rows\",\n",
    "    layout=widgets.Layout(width=\"60%\"),\n",
    ")\n",
    "rows_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Variability of property values\n",
    "\n",
    "In the first chart we can see which properties have relatively little variability (near zero)\n",
    "and properties that have different values for many or most failed requests. \n",
    "\n",
    "We can note that the IP address field shows a lot of variability for our failed attempts.\n",
    "It is common for password spray attacks to cycle through addresses to try to avoid\n",
    "simple brute force detections.\n",
    "\n",
    "In the second chart we are plotting the relative variability of failed vs. success events.\n",
    "If the properties have about the same level of variability in failed and success the \n",
    "score in the second chart will be close to 1.\n",
    "\n",
    "Some properties where we often see significantly less variability in failed vs. success\n",
    "logons:\n",
    "\n",
    "- device_str (this is a flattened version of the dynamic JSON field DeviceDetails). It\n",
    "  contains data from the user agent (UA) such as OS version and type.\n",
    "- UserAgent\n",
    "- ResourceDisplayName\n",
    "- AppID and AppDisplayName - there is a one-to-one correspondence between these two,\n",
    "  so these are essentially the same single property.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:54:00.027100Z",
     "start_time": "2020-08-24T01:53:59.827106Z"
    }
   },
   "outputs": [],
   "source": [
    "failed_subset_df = failed_subset.reset_index().iloc[: rows_select.value]\n",
    "cols = list(failed_subset_df.columns)\n",
    "cols.remove(\"Count\")\n",
    "failed_check_vars = failed_signin_list_df.merge(failed_subset_df, on=cols)\n",
    "\n",
    "\n",
    "def get_unique_prop(ser):\n",
    "    \"\"\"Return the relative uniqueness of values for each property.\"\"\"\n",
    "    try:\n",
    "        return len(ser.unique()) / len(ser)\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Calculate the relative variability of success vs. failure properties\n",
    "auth_variability_df = (\n",
    "    pd.DataFrame(\n",
    "        data=[\n",
    "            failed_check_vars.drop(columns=[\"TimeGenerated\"]).apply(\n",
    "                lambda x: get_unique_prop(x)\n",
    "            ),\n",
    "            success_signin_list_df.drop(columns=[\"TimeGenerated\"]).apply(\n",
    "                lambda x: get_unique_prop(x)\n",
    "            ),\n",
    "        ],\n",
    "        index=[\"failed\", \"success\"],\n",
    "    )\n",
    "    .dropna(axis=1)\n",
    "    .T\n",
    ")\n",
    "\n",
    "del failed_check_vars\n",
    "\n",
    "# Plot a bar graph showing variability in failed logons\n",
    "failed_feat_props_fig = (auth_variability_df[[\"failed\"]]).hvplot.barh(\n",
    "    title=\"Properties - variability in failed logons\",\n",
    "    rot=90,\n",
    "    height=400,\n",
    "    width=600,\n",
    "    color=\"red\",\n",
    ")\n",
    "success_feat_props_fig = (auth_variability_df[[\"success\"]]).hvplot.barh(\n",
    "    title=\"Properties - variability in success logons\",\n",
    "    rot=90,\n",
    "    height=400,\n",
    "    width=600,\n",
    ")\n",
    "\n",
    "# Plot the ratio of variability in success vs. failed\n",
    "md(\n",
    "    \"The columns in the second plot with a high value show repetition not typical of successful logons\",\n",
    "    \"large\",\n",
    ")\n",
    "comp_feat_props_fig = (\n",
    "    auth_variability_df[\"success\"] / auth_variability_df[\"failed\"]\n",
    ").hvplot.barh(\n",
    "    title=\"Properties - ratio of variability in success / failed.\",\n",
    "    rot=90,\n",
    "    height=400,\n",
    "    width=600,\n",
    "    color=\"orange\",\n",
    ")\n",
    "display(failed_feat_props_fig + success_feat_props_fig + comp_feat_props_fig)\n",
    "md(\n",
    "    \"\"\"\n",
    "<br>\n",
    "<b>Ratio of property variability for success/failed events</b><br>\n",
    "Numbers > 1 means that there is more variability in successful logins than\n",
    "failed attempts\n",
    "\"\"\"\n",
    ")\n",
    "display(\n",
    "    pd.DataFrame(\n",
    "        auth_variability_df[\"success\"] / auth_variability_df[\"failed\"],\n",
    "        columns=[\"Ratio\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "wrn_not_enough_repeats = \"\"\"\n",
    "<p style='background-color: gold; font-size: medium'>\n",
    "Not enough properties with repetitive values for this section to work.\n",
    "Try with a different subset of input events (fewer ResultTypes)\n",
    "</p>\n",
    "\"\"\"\n",
    "if (\n",
    "    len(\n",
    "        auth_variability_df[\n",
    "            (auth_variability_df[\"success\"] / auth_variability_df[\"failed\"]) > 2\n",
    "        ]\n",
    "    )\n",
    "    < 3\n",
    "):\n",
    "    md(wrn_not_enough_repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "We may see that the failed requests repeated several of the request parameters.\n",
    "These commonly include:\n",
    "- UserPrincipalName\n",
    "- UserAgent\n",
    "- AppId\n",
    "- IPAddress\n",
    "\n",
    "Lots of repetition in failed requests points to logons caming from an automated source.\n",
    "\n",
    "> **Note**: the \"automated source\" may not be an attacker - it could\n",
    "> just be some forgotten device or service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of failure logons with repeating properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:54:19.186235Z",
     "start_time": "2020-08-24T01:54:19.151235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the columns with biggest delta\n",
    "delta_threshold = 2.0\n",
    "repeating_cols = list(\n",
    "    auth_variability_df[\n",
    "        (auth_variability_df[\"success\"] / auth_variability_df[\"failed\"])\n",
    "        > delta_threshold\n",
    "    ].index.values\n",
    ")\n",
    "\n",
    "\n",
    "# Group by these columns to find the repeating values\n",
    "repeating_fail_props = (\n",
    "    failed_signin_list_df[[\"TimeGenerated\", *repeating_cols]]\n",
    "    .groupby(repeating_cols)\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"TimeGenerated\": \"Count\"})\n",
    ")\n",
    "# if \"UserPrincipalName\" in repeating_cols:\n",
    "#     repeating_fail_props = repeating_fail_props.assign(UserPrincipalName=lambda x: mask_names(x.UserPrincipalName))\n",
    "# if \"UserId\" in repeating_cols:\n",
    "#     repeating_fail_props = repeating_fail_props.assign(UserId=lambda x: mask_guids(x.UserId))\n",
    "\n",
    "md(\n",
    "    \"Top 20 Login failure patterns matching our 'Low Variability' property values.\",\n",
    "    \"bold\",\n",
    ")\n",
    "rpt_fail_cols = list(repeating_fail_props.columns)\n",
    "rpt_fail_cols.remove(\"Count\")\n",
    "obs_data = (\n",
    "    repeating_fail_props\n",
    "    .assign(UserPrincipalName=lambda x: mask_names(x.UserPrincipalName))\n",
    "    .assign(UserId=lambda x: mask_guids(x.UserId))\n",
    "    .groupby(rpt_fail_cols)\n",
    "    .sum()\n",
    "    .sort_values(\"Count\", ascending=False)\n",
    "    .head(20)\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Sample of failure logons with repeating properties\",\n",
    "    description=\"Top 20 Login failure patterns matching our 'Low Variability' property values.\",\n",
    "    link=fmt_link(\"Sample of failure logons with repeating properties\"),\n",
    "    data=obs_data,\n",
    "    score=1 if len(obs_data) < 5 else 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Logon Count Threshold for Searching for successful logons with similar properties\n",
    "We use the 90th percentile by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold over which we want to search for similar patterns\n",
    "md(\n",
    "    \"Set theshold for repeating values - search for patterns with Count >= threshold\",\n",
    "    \"bold\",\n",
    ")\n",
    "\n",
    "repeat_threshold = widgets.BoundedIntText(\n",
    "    value=int(repeating_fail_props.Count.quantile(q=0.9)),\n",
    "    min=0,\n",
    "    max=repeating_fail_props.Count.max(),\n",
    "    step=1,\n",
    "    description=\"Threshold\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(repeat_threshold)\n",
    "md(\n",
    "    \"If the following query fails/times out increase this threshold (default = 90th percentile)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use suspicious logon properties to check for other activity\n",
    "\n",
    "We can use the values that we see in the anomalous failures to look for other logons using the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build filter list from repeating column values\n",
    "from datetime import timedelta\n",
    "\n",
    "# add IPs from previous query >= threshold to ip_log\n",
    "ips_to_log = (\n",
    "    repeating_fail_props.merge(failed_signin_list_df, on=repeating_cols)\n",
    "    .query(\"Count >= 51\")\n",
    "    .IPAddress.unique()\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ips_to_log,\n",
    "        reason=\"Failed logons with repeating properties\",\n",
    "        section=\"Sample of failure logons with repeating properties\",\n",
    "    )\n",
    ")\n",
    "\n",
    "freq_fail_props = repeating_fail_props[\n",
    "    repeating_fail_props[\"Count\"] >= repeat_threshold.value\n",
    "]\n",
    "\n",
    "suspect_signins_filt = []\n",
    "for _, row in freq_fail_props[repeating_cols].drop_duplicates().iterrows():\n",
    "    # for each row build a filter experssion combining each\n",
    "    # column name and value with \"AND\"\n",
    "    suspect_signins_filt.append(\n",
    "        \" and \".join((f\"{col}=='{row[col]}'\" for col in repeating_cols))\n",
    "    )\n",
    "\n",
    "# Join the fitler list with \"OR\"\n",
    "where_suspect = \" or \".join(f\"({filt_item})\" for filt_item in suspect_signins_filt)\n",
    "where_suspect\n",
    "# Search from start of anomalies + 5 days\n",
    "start = failed_signin_list_df.TimeGenerated.min()\n",
    "end = start + timedelta(5)\n",
    "\n",
    "# Create and run the query\n",
    "qry = \"\"\"\n",
    "SigninLogs\n",
    "| project {field_list}\n",
    "| where TimeGenerated >= datetime({start}) and TimeGenerated <= datetime({end})\n",
    "| extend device_str = tostring(DeviceDetail)\n",
    "| where {suspect_filter}\n",
    "| where ResultType == 0\n",
    "\"\"\"\n",
    "print(\"Querying for success logins with matching properties...\")\n",
    "logins_to_investigate = qry_prov.exec_query(\n",
    "    qry.format(\n",
    "        suspect_filter=where_suspect, start=start, end=end, field_list=project_fields\n",
    "    )\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successful logons with suspicious login properties\n",
    "Our results from the query show some successful logons (which we should investigate)\n",
    "The majority though, are different types of failures - some even blocked because of\n",
    "a known malicious IP source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:54:31.003990Z",
     "start_time": "2020-08-24T01:54:30.977989Z"
    }
   },
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    \"ResourceDisplayName\",\n",
    "    \"AppDisplayName\",\n",
    "    \"UserAgent\",\n",
    "    \"ResultType\",\n",
    "    \"ResultDescription\",\n",
    "]\n",
    "\n",
    "if logins_to_investigate.empty:\n",
    "    md(\"No successful logins found with matching properties.\")\n",
    "else:\n",
    "    md(\n",
    "        \"<p style='background-color: coral; font-size: large'>Successful logins found with matching properties.</p>\"\n",
    "    )\n",
    "    obs_data = (\n",
    "        logins_to_investigate[[\"Location\", *group_cols]]\n",
    "        .groupby(group_cols)\n",
    "        .agg(\n",
    "            logon_count=pd.NamedAgg(\"Location\", \"count\"),\n",
    "            location_count=pd.NamedAgg(\"Location\", pd.Series.nunique),\n",
    "        )\n",
    "    )\n",
    "    display(obs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_mssg = \"\"\"\n",
    "<hr>\n",
    "<p style='background-color: coral; font-size: large; margin: 5pt'>These logins should be investigated.</p>\n",
    "<hr>\n",
    "\"\"\"\n",
    "if not logins_to_investigate.empty:\n",
    "    md(warning_mssg)\n",
    "    if \"UserPrincipalName\" in group_cols:\n",
    "        logins_to_investigate = logins_to_investigate.assign(\n",
    "            UserPrincipalName=lambda x: mask_names(x.UserPrincipalName)\n",
    "        )\n",
    "    if \"UserId\" in group_cols:\n",
    "        logins_to_investigate = logins_to_investigate.assign(\n",
    "            UserId=lambda x: mask_guids(x.UserId)\n",
    "        )\n",
    "    obs_data = logins_to_investigate.groupby(\n",
    "        [\"UserPrincipalName\", \"UserAgent\", \"AppDisplayName\"]\n",
    "    ).agg(\n",
    "        logon_count=pd.NamedAgg(\"Location\", \"count\"),\n",
    "        location_count=pd.NamedAgg(\"Location\", pd.Series.nunique),\n",
    "        start=pd.NamedAgg(\"TimeGenerated\", \"min\"),\n",
    "        end=pd.NamedAgg(\"TimeGenerated\", \"max\"),\n",
    "    )\n",
    "    display(obs_data)\n",
    "    obs_log.add_observation(\n",
    "        caption=\"*Successful logons with suspicious login properties\",\n",
    "        description=\"Success logins matching our 'Low Variability' property values.\",\n",
    "        data=obs_data,\n",
    "        link=fmt_link(\"Successful logons with suspicious login properties\"),\n",
    "        score=5,\n",
    "    )\n",
    "    ip_log.extend(\n",
    "        create_ip_entries(\n",
    "            ip_addresses=logins_to_investigate.IPAddress.unique(),\n",
    "            reason=\"Successful login with suspicious repeated properties\",\n",
    "            section=\"Successful logons with suspicious login properties\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Office Activity from suspicious IP Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any IPs from successful logins\n",
    "suspicious_ips = list(logins_to_investigate.IPAddress.unique())\n",
    "# Add the IPs from our original failure list\n",
    "if \"IPAddress\" in freq_fail_props:\n",
    "    suspicious_ips.extend(list(freq_fail_props.IPAddress.unique()))\n",
    "\n",
    "suspicious_ips_list = \", \".join([f'\"{ip}\"' for ip in suspicious_ips])\n",
    "if suspicious_ips_list:\n",
    "\n",
    "    office_activity = qry_prov.Office365.list_activity_for_ip(\n",
    "        start=start, end=end, ip_address_list=suspicious_ips, split_query_by=\"1d\"\n",
    "    )\n",
    "    if not office_activity.empty:\n",
    "        md(\n",
    "            \"<p style='background-color: gold' >Consider investigating the following activity</p>\"\n",
    "        )\n",
    "    obs_data = (\n",
    "        office_activity[\n",
    "            [\"TimeGenerated\", \"OfficeWorkload\", \"Operation\", \"UserId\", \"ClientIP\"]\n",
    "        ]\n",
    "        .assign(UserId=lambda x: mask_names(x.UserId))\n",
    "        .groupby([\"UserId\", \"ClientIP\", \"OfficeWorkload\"])\n",
    "        .agg(\n",
    "            events=pd.NamedAgg(\"TimeGenerated\", \"count\"),\n",
    "            start=pd.NamedAgg(\"TimeGenerated\", \"min\"),\n",
    "            end=pd.NamedAgg(\"TimeGenerated\", \"max\"),\n",
    "            operations=pd.NamedAgg(\n",
    "                \"Operation\", aggfunc=lambda x: sorted(x.unique().tolist())\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    display(obs_data)\n",
    "    obs_log.add_observation(\n",
    "        caption=\"Office Activity from suspicious IP Addresses\",\n",
    "        description=\"Office activity with IPs matching our 'Low Variability' property values.\",\n",
    "        data=obs_data,\n",
    "        link=fmt_link(\"Office Activity from suspicious IP Addresses\"),\n",
    "        score=5,\n",
    "    )\n",
    "    ip_log.extend(\n",
    "        create_ip_entries(\n",
    "            ip_addresses=office_activity.ClientIP.unique(),\n",
    "            reason=\"Successful login with suspicious repeated properties\",\n",
    "            section=\"Successful logons with suspicious login properties\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del office_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use suspicious logon properties to check for other activity \n",
    "### (excluding Username/ID/IPAddress)\n",
    "\n",
    "We can use the values that we see in the anomalous failures to look for other logons using the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build filter list from repeating column values\n",
    "from datetime import timedelta\n",
    "\n",
    "excluded_cols = [\"UserPrincipalName\", \"UserId\", \"IPAddress\"]\n",
    "freq_fail_props = repeating_fail_props[\n",
    "    repeating_fail_props[\"Count\"] > repeat_threshold.value\n",
    "]\n",
    "\n",
    "no_user_cols = list(set(repeating_cols) - set(excluded_cols))\n",
    "no_user_cols\n",
    "suspect_signins_filt = []\n",
    "for _, row in freq_fail_props[no_user_cols].drop_duplicates().iterrows():\n",
    "    # for each row build a filter experssion combining each\n",
    "    # column name and value with \"AND\"\n",
    "    suspect_signins_filt.append(\n",
    "        \" and \".join((f\"{col}=='{row[col]}'\" for col in no_user_cols))\n",
    "    )\n",
    "\n",
    "# Join the filter list with \"OR\"\n",
    "where_suspect = \" or \".join(f\"({filt_item})\" for filt_item in suspect_signins_filt)\n",
    "\n",
    "# Search from start of anomalies + 5 days\n",
    "start = failed_signin_list_df.TimeGenerated.min()\n",
    "end = start + timedelta(5)\n",
    "# Create and run the query\n",
    "qry = \"\"\"\n",
    "SigninLogs\n",
    "| project {field_list}\n",
    "| where TimeGenerated >= datetime({start}) and TimeGenerated <= datetime({end})\n",
    "| extend device_str = tostring(DeviceDetail)\n",
    "| where {suspect_filter}\n",
    "| where ResultType == 0\n",
    "\"\"\"\n",
    "logins_to_investigate_no_user = None\n",
    "try:\n",
    "    logins_to_investigate_no_user = qry_prov.exec_query(\n",
    "        qry.format(\n",
    "            suspect_filter=where_suspect,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            field_list=project_fields,\n",
    "        )\n",
    "    )\n",
    "\n",
    "except:\n",
    "    md_warn(\"Query failed - try with higher logon threshold (above)\")\n",
    "\n",
    "if (\n",
    "    logins_to_investigate_no_user is not None\n",
    "    and not logins_to_investigate_no_user.empty\n",
    "):\n",
    "    group_cols = [\n",
    "        \"ResourceDisplayName\",\n",
    "        \"AppDisplayName\",\n",
    "        \"UserAgent\",\n",
    "        \"ResultType\",\n",
    "        \"ResultDescription\",\n",
    "    ]\n",
    "\n",
    "    obs_data = (\n",
    "        logins_to_investigate_no_user.assign(\n",
    "            UserPrincipalName=lambda x: mask_names(x.UserPrincipalName),\n",
    "            UserId=lambda x: mask_guids(x.UserId),\n",
    "            IPAddress=lambda x: mask_ips(x.IPAddress)\n",
    "        )\n",
    "        .groupby([*group_cols, *excluded_cols])\n",
    "        .agg(\n",
    "            logon_count=pd.NamedAgg(\"Location\", \"count\"),\n",
    "            location_count=pd.NamedAgg(\"Location\", pd.Series.nunique),\n",
    "        )\n",
    "    )\n",
    "    md(warning_mssg)\n",
    "    display(obs_data)\n",
    "    obs_log.add_observation(\n",
    "        caption=\"*Successful logons from sources using suspicious login properties\",\n",
    "        description=\"Similar to earlier query but excluding matching on name or IP.\",\n",
    "        data=obs_data,\n",
    "        link=fmt_link(\"Use suspicious logon properties to check for other activity\"),\n",
    "        score=3,\n",
    "    )\n",
    "    ip_log.extend(\n",
    "        create_ip_entries(\n",
    "            ip_addresses=logins_to_investigate_no_user.IPAddress.unique(),\n",
    "            reason=\"Successful login with suspicious repeated properties\",\n",
    "            section=\"Use suspicious logon properties to check for other activity\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Properties with a comparative variablility score close to 1 mean that failed requests have little to distinguish them from successful requests.\n",
    "Properties with much higher scores mean that legitimate requests are much more variable.\n",
    "\n",
    "The columns with repetitious values in the **failed logons** data are:\n",
    "- AppDisplayName\n",
    "- AppId (1:1 correspondence with AppDisplayName\n",
    "- ResourceDisplayName\n",
    "- device_str\n",
    "\n",
    "We can use requests with repeated values for the properties to spot potential password spray and other brute-force attacks.\n",
    "```\n",
    "    SigninLogs\n",
    "    | extend device_str = tostring(DeviceDetail)\n",
    "    | where (susp_col1 == \"susp_val1\" and \"col2\" == \"susp_val2\" ....)\n",
    "        or (susp_col1 == \"susp_val1\" and \"col2\" == \"susp_val2\" ....)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.x.1 Looking at the time difference between login requests\n",
    "\n",
    "> **Note** although potentially interesting this item isn't used in further analysis in the notebook.\n",
    "\n",
    "We'd expect automated attacks to show short and regular intervals between requests. Even randomly delayed/spaced requests will likely exhibit some pattern over time.\n",
    "\n",
    "We may see some difference in the timing between successive events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Separation between Failed Logons vs Success Logons \n",
    "\n",
    "Plot of a histograms of the delay between requests. \n",
    "There is a strong peak\n",
    "show that many of the failed attempts occured over a short period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:55:11.619139Z",
     "start_time": "2020-08-24T01:55:11.443140Z"
    }
   },
   "outputs": [],
   "source": [
    "time_diffs_f = (\n",
    "    failed_signin_list_df.sort_values(\"TimeGenerated\")[\"TimeGenerated\"]\n",
    "    .diff(1)\n",
    "    .dt.microseconds.dropna()\n",
    ")\n",
    "time_diffs_s = (\n",
    "    success_signin_list_df.sort_values(\"TimeGenerated\")[\"TimeGenerated\"]\n",
    "    .diff(1)\n",
    "    .dt.microseconds.dropna()\n",
    ")\n",
    "(\n",
    "    time_diffs_f[time_diffs_f < 1000000000].hvplot.hist(\n",
    "        bins=np.linspace(0, 1000000, 35),\n",
    "        rot=90,\n",
    "        title=\"Failed logons\",\n",
    "        height=400,\n",
    "        width=400,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    + time_diffs_s[time_diffs_s < 1000000000].hvplot.hist(\n",
    "        bins=np.linspace(0, 1000000, 35),\n",
    "        rot=90,\n",
    "        title=\"Success logons\",\n",
    "        height=400,\n",
    "        width=400,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.x.2 Use of Consecutive or Closely Clustered IP addresses\n",
    "\n",
    "> **Note** although potentially interesting this item isn't used in further analysis in the notebook.\n",
    "\n",
    "We noted earlier that password spray attackers tend to change the source IP address\n",
    "after one or a small number of requests.\n",
    "Something we can look for is whether the numerical spacing between the failing IP addresses\n",
    "is noticably different to that that we find for legitimate users. \n",
    "\n",
    "To do this we need to \n",
    "- convert the IP addresses into numbers (the 32-bit integer\n",
    "  equivalent of the IP Address). \n",
    "- calculate the numerical difference\n",
    "  between the IP addresses in successive events using pandas.diff() function.\n",
    "  This will show us how much successive requests are differing.\n",
    "\n",
    "You would expect this difference between successive requests\n",
    "to be fairly large and random if the sources are users logging in from offices, home,\n",
    "customer sites, hotels, etc. If an automated attack is randomly picking IP addresses\n",
    "from small pools of available IPs it might look more homogeneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:56:02.744695Z",
     "start_time": "2020-08-24T01:56:02.504694Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipaddress import ip_address\n",
    "\n",
    "ip_cols = [\"TimeGenerated\", \"IPAddress\"]\n",
    "failed_ips = failed_signin_list_df[ip_cols].copy()\n",
    "success_ips = success_signin_list_df.copy()\n",
    "\n",
    "md(\n",
    "    \"\"\"\n",
    "    If IPs for failed logons are unique or very similar,\n",
    "    most will be clustered near 0\n",
    "\"\"\",\n",
    "    \"bold\",\n",
    ")\n",
    "display(\n",
    "    np.log10(failed_ips[\"IPAddress\"].value_counts()).hvplot.hist(\n",
    "        title=\"(log10) Number of times IP address are re-used in failed and success logins\",\n",
    "        bins=100,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    + np.log10(success_ips[\"IPAddress\"].value_counts()).hvplot.hist(\n",
    "        bins=100,\n",
    "        color=\"blue\",\n",
    "    )\n",
    ")\n",
    "\n",
    "failed_ips[\"ip_num\"] = failed_ips[\"IPAddress\"].apply(\n",
    "    lambda x: int.from_bytes(ip_address(x).packed, byteorder=\"big\")\n",
    ")\n",
    "\n",
    "# Plot timeline of numerical differences between consecutive IPs\n",
    "clip_val = 10 ** 20\n",
    "failed_ip_diff_df = pd.concat(\n",
    "    [\n",
    "        failed_ips.sort_values(\"TimeGenerated\")[\"TimeGenerated\"],\n",
    "        failed_ips.sort_values(\"TimeGenerated\")[\"ip_num\"]\n",
    "        .diff(1)\n",
    "        .fillna(0)\n",
    "        .astype(\"float64\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ").dropna()\n",
    "failed_ip_diff_df.loc[failed_ip_diff_df[\"ip_num\"] != 0, \"ip_num\"] = np.log10(\n",
    "    np.abs(failed_ip_diff_df[failed_ip_diff_df[\"ip_num\"] != 0].ip_num)\n",
    ").fillna(0)\n",
    "success_ips = success_signin_list_df.copy()\n",
    "\n",
    "# print(\"Some IP address are repeated but most are unique.\")\n",
    "# print(success_ips.sort_values(\"IPAddress\")[\"IPAddress\"].value_counts())\n",
    "# Convert IP Address to integers\n",
    "success_ips[\"ip_num\"] = success_ips[\"IPAddress\"].apply(\n",
    "    lambda x: int.from_bytes(ip_address(x).packed[:3], byteorder=\"big\")\n",
    ")\n",
    "\n",
    "# Plot timeline of numerical differences between consecutive IPs\n",
    "success_ip_diff_df = pd.concat(\n",
    "    [\n",
    "        success_ips.sort_values(\"TimeGenerated\")[\"TimeGenerated\"],\n",
    "        success_ips.sort_values(\"TimeGenerated\")[\"ip_num\"]\n",
    "        .diff(1)\n",
    "        .fillna(0)\n",
    "        .astype(\"float64\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ").dropna()\n",
    "success_ip_diff_df.loc[success_ip_diff_df[\"ip_num\"] != 0, \"ip_num\"] = np.log10(\n",
    "    np.abs(success_ip_diff_df[success_ip_diff_df[\"ip_num\"] != 0].ip_num)\n",
    ").fillna(0)\n",
    "\n",
    "# Plot the graphs\n",
    "md(\n",
    "    \"\"\"\n",
    "    These time graphs show the numeric differences between successive\n",
    "    IP addresses for failed (orange) and success (blue) logins\n",
    "    \"\"\",\n",
    "    \"bold\",\n",
    ")\n",
    "display(\n",
    "    failed_ip_diff_df.hvplot.hist(\n",
    "        bins=500, line_color=\"orange\", title=\"Histogram of IP differences\"\n",
    "    )\n",
    "    * success_ip_diff_df.hvplot.hist(bins=500, line_color=\"blue\")\n",
    ")\n",
    "md(\n",
    "    \"\"\"\n",
    "    Timeline of  IP numeric differences\n",
    "    (y axis = IP numeric value)\n",
    "    \"\"\",\n",
    "    \"bold\",\n",
    ")\n",
    "display(\n",
    "    failed_ip_diff_df.hvplot.scatter(\n",
    "        title=\"Timeline - Success/Failed logon IP diff between successive attempts\",\n",
    "        x=\"TimeGenerated\",\n",
    "        color=\"orange\",\n",
    "        size=3,\n",
    "    )\n",
    "    * success_ip_diff_df.hvplot.scatter(\n",
    "        x=\"TimeGenerated\", color=\"blue\", size=1, alpha=0.25\n",
    "    )\n",
    ")\n",
    "del success_ips, failed_ips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Failed logons are generally come from a consistent IP space but show intermittent IP sources\n",
    "  that have very different values.\n",
    "- Success logons tend to be clustered more homogenously\n",
    "- However, the distribution of the vast majority of login failure IPs looks very simlar\n",
    "  to the success logins\n",
    "- While an interesting observation, it's difficult to picture how to use this data in further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A formula\n",
    "\n",
    "Although obvious from the graphic, it would be nice to see this numerically.\n",
    "We can use the standard deviation of the suspect IP address.\n",
    "\n",
    "If the origin of the suspect IP addresses is users forgetting and retry their\n",
    "passwords the variance of these divided by the variance of the successful\n",
    "logins should be close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T01:59:22.690950Z",
     "start_time": "2020-08-24T01:59:22.670947Z"
    }
   },
   "outputs": [],
   "source": [
    "std_fail = failed_ip_diff_df.ip_num.std()\n",
    "std_success = success_ip_diff_df.ip_num.std()\n",
    "md(f\"Stddev of failure IPs is {std_fail:f}\")\n",
    "md(f\"Stddev of success IPs is {std_success:f}\")\n",
    "md(f\"Ratio of fail/success stddev IPs is {std_fail/std_success:f}\", \"bold\")\n",
    "md(\n",
    "    \"(expected is close to 1.0 if failures followed the same pattern as success source IPs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Comparing IP Blocks for Failed and Success Logons\n",
    "\n",
    "We can reuse our integer representation of the IP Addresses to do \n",
    "some simple scatter plots. In this case though, we \"aggregate\" the IP\n",
    "Addresses into blocks by only using the upper 3 bytes of the 4 byte address.\n",
    "So 192.168.1.23 and 192.168.1.55 would both appear in the 192.168.1.0 block.\n",
    "\n",
    "The data below shows that there are\n",
    "distinct blocks of failed logon source IP addresses that are very different\n",
    "to the majority of success logons. If these were genuine logon failures from \n",
    "users we would expect to see the failures occupying the approximately\n",
    "the same address space as the successful logons.\n",
    "\n",
    "> **NOTE** the scaling of the graph suggests that many of the failing\n",
    "> IP addresses are using the same netblock as successful IPs but if you use\n",
    "> the graph zoom controls to zoom in to particular areas you will\n",
    "> usually see that they do not overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get a representative sample of org's normal IP usage\n",
    "# pick a day near end of selection period - pick Wed to avoid Monday holidays and varying weekends (e.g. Israel)\n",
    "start_rng = pd.Timestamp(query_time.end) - pd.Timedelta(\"7d\")\n",
    "wednesday = [\n",
    "    day for day in pd.date_range(start_rng, periods=7, freq=\"1D\") if day.dayofweek == 2\n",
    "][0]\n",
    "thursday = [\n",
    "    day for day in pd.date_range(start_rng, periods=7, freq=\"1D\") if day.dayofweek == 3\n",
    "][0]\n",
    "\n",
    "\n",
    "# Get summary statistics for IP block usage\n",
    "print(\"Querying success login IP statistics\")\n",
    "qry = f\"\"\"\n",
    "SigninLogs\n",
    "| where TimeGenerated > datetime({wednesday}) and TimeGenerated < datetime({thursday})\n",
    "| where ResultType == 0\n",
    "| extend IPBlock_str = iff(\n",
    "    IPAddress contains(\".\"), \n",
    "    strcat_array(array_slice(split(IPAddress, \".\"), 0, 2), \".\"), // IPv4 and v6/v4\n",
    "    strcat_array(array_slice(split(IPAddress, \":\"), 0, 3), \":\")\n",
    "    )\n",
    "| summarize Count = count() by IPBlock_str\n",
    "\"\"\"\n",
    "\n",
    "file_name = archive_name(\"ip_block_usage\", wednesday, thursday)\n",
    "if not use_caching() or not archive_exists(file_name):\n",
    "    print(\"querying data...\")\n",
    "    ip_block_usage = qry_prov.exec_query(qry)\n",
    "    save_archive(file_name, ip_block_usage)\n",
    "else:\n",
    "    ip_block_usage = get_archive(file_name)\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "# Calculate ip block usage figure\n",
    "def calc_ipblock_usage(ip_df):\n",
    "    \"\"\"Give a logarithmic score to number of times block is used.\"\"\"\n",
    "    ip_df[\"usage\"] = np.floor(np.log(ip_df[\"Count\"])) + 1\n",
    "\n",
    "\n",
    "print(\"Calculating IP Block useage for success logins\")\n",
    "calc_ipblock_usage(ip_block_usage)\n",
    "\n",
    "\n",
    "# Merge with Success logon usage to get netblock usage for the failed logon attempts\n",
    "failed_signin_list_df[\"IPBlock_str\"] = failed_signin_list_df.IPAddress.str.extract(\n",
    "    \"(\\d+\\.\\d+.\\d+)\", expand=False\n",
    ").fillna(\"1.1.1\")\n",
    "failed_signin_usage = failed_signin_list_df.merge(\n",
    "    ip_block_usage[[\"IPBlock_str\", \"usage\"]], on=\"IPBlock_str\", how=\"left\"\n",
    ")\n",
    "failed_signin_usage.fillna(0, inplace=True)\n",
    "print(\"done\")\n",
    "\n",
    "md(\"<br>Usage Scoring\", \"bold\")\n",
    "md(\n",
    "    \"The 'usage' score is the log10 of the number of successful logons for an IP Block.<br>\"\n",
    ")\n",
    "\n",
    "bins = (ip_block_usage.usage.nunique() + 1) * 2\n",
    "display(\n",
    "    failed_signin_usage[\"usage\"].hvplot.hist(\n",
    "        bins=bins,\n",
    "        color=\"orange\",\n",
    "        title=\"Number of failed logins by known IP block usage\",\n",
    "    )\n",
    "    + ip_block_usage[\"usage\"].hvplot.hist(\n",
    "        bins=bins,\n",
    "        line_color=\"blue\",\n",
    "        title=\"Number of success logins by known IP block usage\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed logons from IP addresses (blocks) not used by Organization (Top 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_data = (\n",
    "    failed_signin_usage[failed_signin_usage[\"usage\"] == 0]\n",
    "    .groupby([*group_cols, \"IPAddress\"])\n",
    "    .agg(\n",
    "        logon_count=pd.NamedAgg(\"Location\", \"count\"),\n",
    "        location_count=pd.NamedAgg(\"Location\", \"nunique\"),\n",
    "    )\n",
    "    .sort_values(\"logon_count\", ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "\n",
    "# We're resetting and setting the index again so that we can\n",
    "# mask the IP data\n",
    "obs_data = (\n",
    "    signin_data.reset_index()\n",
    "    .assign(IPAddress=lambda x: mask_ips(x.IPAddress))\n",
    "    .groupby([*group_cols, \"IPAddress\"])\n",
    "    .agg({\"logon_count\": \"sum\", \"location_count\": \"sum\"})\n",
    "    .sort_values(\"logon_count\", ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logons from IPs blocks not used by organization\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\n",
    "        \"Failed logons from IP addresses (blocks) not used by Organization (Top 30)\"\n",
    "    ),\n",
    "    score=2,\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=signin_data.reset_index().IPAddress.unique(),\n",
    "        reason=\"Failed logons from IPs blocks not used by organization\",\n",
    "        section=\"Failed logons from IP addresses (blocks) not used by Organization (Top 30)\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed logons from IP blocks rarely used by organization (Top 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_data = (\n",
    "    failed_signin_usage[failed_signin_usage[\"usage\"].isin((1, 2))]\n",
    "    .groupby([*group_cols, \"IPBlock_str\", \"IPAddress\"])\n",
    "    .agg(\n",
    "        logon_count=pd.NamedAgg(\"Location\", \"count\"),\n",
    "        location_count=pd.NamedAgg(\"Location\", \"nunique\"),\n",
    "    )\n",
    "    .sort_values(\"logon_count\", ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "# We're resetting and setting the index again so that we can\n",
    "# mask the IP data\n",
    "obs_data = (\n",
    "    signin_data.reset_index()\n",
    "    .assign(IPAddress=lambda x: mask_ips(x.IPAddress))\n",
    "    .groupby([*group_cols, \"IPAddress\"])\n",
    "    .agg({\"logon_count\": \"sum\", \"location_count\": \"sum\"})\n",
    "    .sort_values(\"logon_count\", ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logons from IPs blocks rarely used (score of 1 or 2) by organization\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"Failed logons from IP blocks rarely used by organization (Top 30)\"),\n",
    "    score=2,\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=signin_data.reset_index().IPAddress.unique(),\n",
    "        reason=\"Failed logons from IPs blocks rarely used by organization\",\n",
    "        section=\"Failed logons from IP blocks rarely used by organization (Top 30)\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed logons from most commonly used IP blocks (Top 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_data = (\n",
    "    failed_signin_usage[failed_signin_usage[\"usage\"] > 2]\n",
    "    .groupby([*group_cols, \"IPBlock_str\", \"IPAddress\"])\n",
    "    .agg(\n",
    "        logon_count=pd.NamedAgg(\"Location\", \"count\"),\n",
    "        location_count=pd.NamedAgg(\"Location\", \"nunique\"),\n",
    "        mean_usage=pd.NamedAgg(\"usage\", \"mean\"),\n",
    "    )\n",
    "    .sort_values(\"logon_count\", ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "# We're resetting and setting the index again so that we can\n",
    "# mask the IP data\n",
    "obs_data = (\n",
    "    signin_data.reset_index()\n",
    "    .assign(IPAddress=lambda x: mask_ips(x.IPAddress))\n",
    "    .groupby([*group_cols, \"IPAddress\"])\n",
    "    .agg({\"logon_count\": \"sum\", \"location_count\": \"sum\"})\n",
    "    .sort_values(\"logon_count\", ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logons from IPs blocks commonly used by organization\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"Failed logons from most commonly used IP blocks (Top 30)\"),\n",
    "    score=1,\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=signin_data.reset_index().query(\"logon_count > 10\").IPAddress.unique(),\n",
    "        reason=\"Failed logons from IPs blocks commonly used by organization\",\n",
    "        section=\"Failed logons from most commonly used IP blocks (Top 30)\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeplot for failed logon IP Blocks (showing usage relative to success logons)\n",
    "Use the legend to deselect usage/ResultType to focus on the remaining logon failures.\n",
    "> Note deselecting doesn't hide the deleselect points, it just makes them more transparent<br>\n",
    "> When many points are overlaid at the same location the color will strengthen\n",
    "\n",
    "If you have a lot of data to display the graphics can be difficult to interpret.\n",
    "Use the resultType selection list to display subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_res_type.value = list(select_res_type.options.values())\n",
    "select_res_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T02:10:52.092193Z",
     "start_time": "2020-08-24T02:10:51.558196Z"
    }
   },
   "outputs": [],
   "source": [
    "fail_res_type_filter = failed_signin_usage[\"ResultType\"].isin(select_res_type.value)\n",
    "\n",
    "\n",
    "def ip_blk_to_num(ip_block):\n",
    "    try:\n",
    "        nums = ip_block.split(\".\")\n",
    "        return int(nums[0]) * 65536 + int(nums[1]) * 256 + int(nums[2])\n",
    "    except AttributeError:\n",
    "        print(ip_block)\n",
    "\n",
    "\n",
    "failed_signin_usage[\"Block_num\"] = failed_signin_usage.apply(\n",
    "    lambda x: ip_blk_to_num(x.IPBlock_str), axis=1\n",
    ")\n",
    "\n",
    "failed_signin_usage[\"TG_num\"] = pd.to_numeric(failed_signin_usage[\"TimeGenerated\"])\n",
    "ip_blk_usage = failed_signin_usage[fail_res_type_filter].hvplot.scatter(\n",
    "    x=\"TG_num\",\n",
    "    y=\"Block_num\",\n",
    "    by=\"usage\",\n",
    "    grid=True,\n",
    "    title=\"IP Timeplot for Failed Logons (key=freq of use in success logins)\",\n",
    "    height=600,\n",
    "    width=600,\n",
    "    hover_cols=[\n",
    "        \"Location\",\n",
    "        \"ResultDescription\",\n",
    "        \"IPAddress\",\n",
    "        \"UserPrincipalName\",\n",
    "        \"IPAddress\",\n",
    "    ],\n",
    "    xlabel=\"time\",\n",
    "    size=8,\n",
    "    muted_alpha=0,\n",
    ")\n",
    "\n",
    "\n",
    "ip_blk_res_type = failed_signin_usage[fail_res_type_filter].hvplot.scatter(\n",
    "    x=\"TG_num\",\n",
    "    y=\"Block_num\",\n",
    "    by=\"ResultType\",\n",
    "    grid=True,\n",
    "    title=\"IP Timeplot for Failed Logons (key=FailureType)\",\n",
    "    height=600,\n",
    "    width=600,\n",
    "    hover_cols=[\n",
    "        \"Location\",\n",
    "        \"ResultDescription\",\n",
    "        \"IPAddress\",\n",
    "        \"UserPrincipalName\",\n",
    "        \"IPAddress\",\n",
    "    ],\n",
    "    xlabel=\"time\",\n",
    "    size=8,\n",
    "    muted_alpha=0,\n",
    ")\n",
    "\n",
    "\n",
    "display(ip_blk_usage + ip_blk_res_type)\n",
    "md(\"ResultType Key\", \"bold\")\n",
    "for res_type in sorted(list(failed_signin_usage[\"ResultType\"].unique())):\n",
    "    md(f\"{res_type:<10} {result_types[res_type]}\".replace(\" \", \"&nbsp;\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record IP Block usage for failed login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_signin_list_df[\"known_ip_block\"] = failed_signin_usage[\"usage\"]\n",
    "del failed_signin_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Failed logons from familiar netblocks tend to be consistent - i.e. it looks as though they are always originating from the same place\n",
    "- There is an overlap in failure types - logins from familiar netblocks tend to be 50126\n",
    "- Netblock origin seems to be a good predictor of an suspicious/failed login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Login Request Timing\n",
    "Can we identify patterns based on timing of logon failures?\n",
    "\n",
    "The goal is to use timing patterns to ID attackers distributed across source IP addresses but using regularly-scheduled requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T18:39:29.687518Z",
     "start_time": "2020-08-26T18:39:29.550519Z"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    (\n",
    "        failed_signin_list_df[\"TimeGenerated\"].dt.minute * 60\n",
    "        + failed_signin_list_df[\"TimeGenerated\"].dt.second\n",
    "    ).hvplot.hist(\n",
    "        bins=3600,\n",
    "        title=\"Histogram of failed logon requests binned by min/sec past hour\",\n",
    "        line_color=\"orange\",\n",
    "    )\n",
    "    + (\n",
    "        success_signin_list_df[\"TimeGenerated\"].dt.minute * 60\n",
    "        + success_signin_list_df[\"TimeGenerated\"].dt.second\n",
    "    ).hvplot.hist(\n",
    "        bins=3600,\n",
    "        title=\"Histogram of success logon requests binned by min/sec past hour\",\n",
    "        line_color=\"blue\",\n",
    "    )\n",
    ")\n",
    "\n",
    "mean_logon_second = len(failed_signin_list_df) / 3600\n",
    "md(f\"Expected count for random distribution is {mean_logon_second:.1f}\")\n",
    "md(\"Significant peaks above this may indicate scheduled logons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peaks in the above chart indicate multiple login attempts occurring at same minute/second past hour\n",
    "Since we'd expect automated logon attempts to repeat some properties we can group by:\n",
    "- AppID\n",
    "- UserAgent\n",
    "- Device Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logon_second = (\n",
    "    failed_signin_list_df[\"TimeGenerated\"].dt.minute * 60\n",
    "    + failed_signin_list_df[\"TimeGenerated\"].dt.second\n",
    ").values\n",
    "\n",
    "# Calculate histogram values\n",
    "counts, freqs = np.histogram(logon_second, bins=3600)\n",
    "\n",
    "# Set a threshold over which we want to search for similar patterns\n",
    "md(\"Default threshold for count of logins at give min/sec past hour\", \"bold\")\n",
    "md(\"default = mean(counts_per_second) + stddev(counts_per_second)\")\n",
    "md(\"Any events with a count above this threshold will be marked as <b>'scheduled'</b>\")\n",
    "count_threshold = widgets.FloatSlider(\n",
    "    value=np.std(counts) + mean_logon_second,\n",
    "    min=0,\n",
    "    max=counts.max(),\n",
    "    step=0.1,\n",
    "    description=\"Threshold\",\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "del logon_second\n",
    "\n",
    "display(count_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_signin_list_df[\"logon_second\"] = (\n",
    "    failed_signin_list_df[\"TimeGenerated\"].dt.minute * 60\n",
    "    + failed_signin_list_df[\"TimeGenerated\"].dt.second\n",
    ")\n",
    "\n",
    "group_cols = [\"AppId\", \"UserAgent\", \"device_str\", \"logon_second\"]\n",
    "\n",
    "# Get the login patterns deemed to be scheduled\n",
    "scheduled_logins = (\n",
    "    failed_signin_list_df[[\"UserPrincipalName\", \"TimeGenerated\", *group_cols]]\n",
    "    .groupby(group_cols)\n",
    "    .agg(\n",
    "        LoginRequests=pd.NamedAgg(\"TimeGenerated\", \"count\"),\n",
    "        UniqueUsers=pd.NamedAgg(\"UserPrincipalName\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .assign(scheduled=True)\n",
    "    .query(\"LoginRequests > @count_threshold.value\")\n",
    ")\n",
    "\n",
    "md(\"Login patterns determined to be scheduled\")\n",
    "obs_data = (\n",
    "    scheduled_logins.groupby([\"AppId\", \"UserAgent\", \"device_str\"])\n",
    "    .agg(\n",
    "        num_fixed_schedules=pd.NamedAgg(\"logon_second\", \"nunique\"),\n",
    "        LoginRequests=pd.NamedAgg(\"LoginRequests\", \"sum\"),\n",
    "        UniqueUsers=pd.NamedAgg(\"UniqueUsers\", \"sum\"),\n",
    "    )\n",
    "    .sort_values(\"LoginRequests\", ascending=False)\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logins that appear scheduled (regular time pulse)\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.3 Login Request Timing\"),\n",
    "    score=1,\n",
    ")\n",
    "\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=(\n",
    "            failed_signin_list_df.merge(\n",
    "                scheduled_logins, on=group_cols\n",
    "            ).IPAddress.unique()\n",
    "        ),\n",
    "        reason=\"Failed logons that happen on regular schedule\",\n",
    "        section=\"2.3 Login Request Timing\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a \"scheduled\" field and assign True for any matching items in\n",
    "# the scheduled_logins DataFrame\n",
    "if \"scheduled\" in failed_signin_list_df.columns:\n",
    "    failed_signin_list_df.drop(columns=\"scheduled\", inplace=True)\n",
    "failed_signin_list_df[\"scheduled\"] = (\n",
    "    failed_signin_list_df[group_cols]\n",
    "    .merge(scheduled_logins, on=group_cols, how=\"left\")\n",
    "    .fillna(False)\n",
    "    .scheduled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We found several repeating patterns based on minute + seconds past the hour\n",
    "- Many of the requests shared the same UserAgent - indicating a common tool and/or source\n",
    "- These requests could be a faulty or forgotten device or application with an old password."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 Using IP location data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample of successful logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:15:07.185558Z",
     "start_time": "2020-08-24T17:14:46.567558Z"
    }
   },
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "SigninLogs\n",
    "| where TimeGenerated > datetime({wednesday}) and TimeGenerated < datetime({thursday})\n",
    "| where ResultType == 0\n",
    "| extend city = tostring(LocationDetails[\"city\"]),\n",
    "  countryOrRegion = tostring(LocationDetails[\"countryOrRegion\"]),\n",
    "  state = tostring(LocationDetails[\"state\"]),\n",
    "  latitude = toint(LocationDetails[\"geoCoordinates\"][\"latitude\"]),\n",
    "  longitude = toint(LocationDetails[\"geoCoordinates\"][\"longitude\"])\n",
    "| summarize loc_count=count() by countryOrRegion, state, city, latitude, longitude\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying success login location statistics...\")\n",
    "file_name = archive_name(\"success_logins_loc\", wednesday, thursday)\n",
    "if not use_caching() or not archive_exists(file_name):\n",
    "    print(\"querying sample of current data...\")\n",
    "    success_logins_loc = qry_prov.exec_query(qry)\n",
    "    save_archive(file_name, success_logins_loc)\n",
    "else:\n",
    "    success_logins_loc = get_archive(file_name)\n",
    "print(\"done.\")\n",
    "\n",
    "# We need to unpack the Location Details field for failure logins\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    tqdm.pandas(desc=\"Records\")\n",
    "\n",
    "\n",
    "def expand_locations(data):\n",
    "    cols = [\"TimeGenerated\", \"IPAddress\", \"ResultType\", \"UserAgent\"]\n",
    "    # Note using apply(pd.Series) here is very slow but simple\n",
    "    print(\"extracting location details..\")\n",
    "    loc_df = data.progress_apply(\n",
    "        lambda x: pd.Series(x[\"LocationDetails\"]), axis=1, result_type=\"expand\"\n",
    "    )\n",
    "    print(\"extracting geo coordinates..\")\n",
    "    geo_df = loc_df.progress_apply(\n",
    "        lambda x: pd.Series(x[\"geoCoordinates\"]), axis=1, result_type=\"expand\"\n",
    "    )\n",
    "    comb_loc_df = pd.concat([loc_df, geo_df], axis=\"columns\")\n",
    "    return pd.concat([data[cols], comb_loc_df], axis=\"columns\")\n",
    "\n",
    "\n",
    "geo_cols = {\n",
    "    \"city\",\n",
    "    \"countryOrRegion\",\n",
    "    \"geoCoordinates\",\n",
    "    \"state\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "}\n",
    "print(\"Expanding location details to columns.\")\n",
    "\n",
    "# Note try to re-use\n",
    "print(\"Processing failed_logins...\\n\")\n",
    "failed_logins_loc = expand_locations(failed_signin_list_df)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the logins spatially\n",
    "If you squint at the graphics you can kind of make out the shape of some of the continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:15:09.594560Z",
     "start_time": "2020-08-24T17:15:09.171560Z"
    }
   },
   "outputs": [],
   "source": [
    "common_args = dict(x=\"longitude\", y=\"latitude\", height=500, width=900)\n",
    "display(\n",
    "    failed_logins_loc.hvplot.scatter(\n",
    "        **common_args,\n",
    "        title=\"Failed vs. Success login locations\",\n",
    "        color=\"orange\",\n",
    "        by=\"ResultType\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "    * success_logins_loc.hvplot.scatter(**common_args, color=\"blue\", alpha=0.3, size=10)\n",
    ")\n",
    "md(\"Success login locations in green.\", \"bold\")\n",
    "md(\n",
    "    \"Note: Fainter green indicates fewer logons, more intense green indicates multiple logons.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show logins with atypical location on a map\n",
    "This shows all login locations that are not common for successful logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:38:18.272120Z",
     "start_time": "2020-08-24T17:38:17.578122Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_ip_entity(row):\n",
    "    ip_ent = entities.IpAddress(Address=row[\"IPAddress\"])\n",
    "    ip_ent.AdditionalData[\"ResultType\"] = row[\"ResultType\"]\n",
    "    ip_ent.AdditionalData[\"UserAgent\"] = row[\"UserAgent\"]\n",
    "    geo_ent = entities.GeoLocation(\n",
    "        Latitude=row[\"latitude\"],\n",
    "        Longitude=row[\"longitude\"],\n",
    "        City=row[\"city\"],\n",
    "        State=row[\"state\"],\n",
    "        CountryName=row[\"countryOrRegion\"],\n",
    "    )\n",
    "    ip_ent.Location = geo_ent\n",
    "    return ip_ent\n",
    "\n",
    "\n",
    "# get logins to investigate from regions not in\n",
    "# success logins\n",
    "other_locs = failed_logins_loc[\n",
    "    ~(failed_logins_loc[\"countryOrRegion\"].isin(success_logins_loc[\"countryOrRegion\"]))\n",
    "    & ~(failed_logins_loc[\"city\"].isin(success_logins_loc[\"city\"]))\n",
    "]\n",
    "folium = FoliumMap(zoom_start=3)\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    folium.add_ip_cluster(\n",
    "        other_locs.apply(create_ip_entity, axis=1).values, color=\"orange\"\n",
    "    )\n",
    "\n",
    "folium.center_map()\n",
    "\n",
    "display(folium)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logons from IPs locations not used by organization\",\n",
    "    data=folium,\n",
    "    link=fmt_link(\"Show logins with atypical location on a map\"),\n",
    "    score=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the numbers of failed logons from atypical locations\n",
    "We can see that the distribution is not the same but it's pretty difficult\n",
    "to see what this means. Let's explore the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:46:23.874292Z",
     "start_time": "2020-08-24T17:46:23.831291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate probabilities of successful logon from a particlular country\n",
    "# for both data sets\n",
    "grouping_cols = [\"countryOrRegion\", \"state\", \"city\"]\n",
    "login_succ_prob = success_logins_loc[[*grouping_cols, \"loc_count\"]]\n",
    "login_succ_prob = (\n",
    "    login_succ_prob.groupby(grouping_cols)\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"loc_count\": \"LoginCount\"})\n",
    ")\n",
    "login_succ_prob[\"LoginProb\"] = (\n",
    "    login_succ_prob.LoginCount / login_succ_prob.LoginCount.sum()\n",
    ")\n",
    "\n",
    "failed_logins_prob = failed_logins_loc[[*grouping_cols, \"ResultType\"]]\n",
    "failed_logins_prob = failed_logins_prob.merge(\n",
    "    login_succ_prob[[*grouping_cols, \"LoginProb\"]],\n",
    "    how=\"left\",\n",
    "    on=[\"countryOrRegion\", \"state\", \"city\"],\n",
    ").fillna(0)\n",
    "failed_logins_loc_comb = failed_logins_loc\n",
    "failed_logins_loc_comb[\"LoginProb\"] = failed_logins_prob[\"LoginProb\"]\n",
    "del failed_logins_prob\n",
    "\n",
    "\n",
    "md(\"Statistics for Successful logins\", \"bold, large\")\n",
    "# Show common and rare locations for Success logons\n",
    "login_locs1 = (\n",
    "    login_succ_prob[[\"countryOrRegion\", \"state\", \"city\", \"LoginProb\"]]\n",
    "    .sort_values(\"LoginProb\", ascending=False)\n",
    "    .head(10)\n",
    "    .to_html()\n",
    ")\n",
    "login_locs1 = f\"<b>Most likely locations for successful logins</b>{login_locs1}\"\n",
    "\n",
    "login_locs2 = (\n",
    "    login_succ_prob[[\"countryOrRegion\", \"state\", \"city\", \"LoginProb\"]]\n",
    "    .sort_values(\"LoginProb\", ascending=False)\n",
    "    .tail(10)\n",
    "    .to_html()\n",
    ")\n",
    "login_locs2 = (\n",
    "    f\"<b>Least likely (non-zero) locations for successful logins</b>{login_locs2}\"\n",
    ")\n",
    "display(widgets.HBox([widgets.HTML(login_locs1), widgets.HTML(login_locs2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    failed_logins_loc_comb[\"LoginProb\"].hvplot.hist(\n",
    "        bins=100,\n",
    "        title=\"Probability distribution for failed logins - likelihood that it came from familiar location\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:54:20.961361Z",
     "start_time": "2020-08-24T17:54:20.663364Z"
    }
   },
   "outputs": [],
   "source": [
    "def col_zero_red(val):\n",
    "    if val == 0:\n",
    "        return f\"background-color: coral\"\n",
    "\n",
    "\n",
    "md(\n",
    "    \"Mean probabilities that failed logins are coming from familiar locations (zero == unseen location)\",\n",
    "    \"bold\",\n",
    ")\n",
    "obs_data = (\n",
    "    failed_logins_loc_comb[\n",
    "        [\n",
    "            \"TimeGenerated\",\n",
    "            \"ResultType\",\n",
    "            \"UserAgent\",\n",
    "            \"countryOrRegion\",\n",
    "            \"state\",\n",
    "            \"LoginProb\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby([\"ResultType\", \"UserAgent\", \"countryOrRegion\", \"state\"])\n",
    "    .agg({\"LoginProb\": \"mean\", \"TimeGenerated\": \"count\"})\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"LoginProb\": \"LocationLoginProbability\",\n",
    "            \"TimeGenerated\": \"FailedLoginCount\",\n",
    "        }\n",
    "    )\n",
    "    .sort_values(\"FailedLoginCount\", ascending=False)\n",
    "    .head(50)\n",
    "    .style.background_gradient(\n",
    "        subset=\"LocationLoginProbability\", cmap=\"Greens\", low=0.0, high=1\n",
    "    )\n",
    "    .applymap(col_zero_red, subset=[\"LocationLoginProbability\"])\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logons list by failure count with login location probability\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"Viewing the numbers of failed logons from atypical locations\"),\n",
    "    score=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:54:20.961361Z",
     "start_time": "2020-08-24T17:54:20.663364Z"
    }
   },
   "outputs": [],
   "source": [
    "md(\"Top failed logins are coming from locations not seen in success logins\", \"bold\")\n",
    "obs_data = (\n",
    "    failed_logins_loc_comb[\n",
    "        [\n",
    "            \"TimeGenerated\",\n",
    "            \"ResultType\",\n",
    "            \"UserAgent\",\n",
    "            \"countryOrRegion\",\n",
    "            \"state\",\n",
    "            \"LoginProb\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby([\"ResultType\", \"UserAgent\", \"countryOrRegion\", \"state\"])\n",
    "    .agg({\"LoginProb\": \"mean\", \"TimeGenerated\": \"count\"})\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"LoginProb\": \"LocationLoginProbability\",\n",
    "            \"TimeGenerated\": \"FailedLoginCount\",\n",
    "        }\n",
    "    )\n",
    "    .query(\"LocationLoginProbability == 0\")\n",
    "    .sort_values(\"FailedLoginCount\", ascending=False)\n",
    "    .head(50)\n",
    "    .style.background_gradient(\n",
    "        subset=\"LocationLoginProbability\", cmap=\"Greens\", low=0.0, high=1\n",
    "    )\n",
    "    .applymap(col_zero_red, subset=[\"LocationLoginProbability\"])\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logons summary from IPs locations not used by organization\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"Viewing the numbers of failed logons from atypical locations\"),\n",
    "    score=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Familiar Location probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_signin_list_df[\"login_loc_prob\"] = failed_logins_loc_comb[\"LoginProb\"]\n",
    "del failed_logins_loc, failed_logins_loc_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Geographical origin of login requests against a normal pattern of the organization\n",
    "can help indicate the probability that the request is bogus.\n",
    "- All categories of failed login came from unlikely locations - with the probability\n",
    "  an order of magnitude smaller than the least likely legitimate login\n",
    "\n",
    "However, there are caveats\n",
    "- the 50125 events for the OneDrive SyncEngine had almost the same probability as\n",
    "  successful logins coming from a familiar (if rare) geographic location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 UserAgent strings\n",
    "UserAgent were recently added to the Azure Active Directory logs. These are potentially more useful\n",
    "than the DeviceDetails field to identify repetitive requests.\n",
    "(DeviceDetails was squashed into the device_str field that we used). DeviceDetails contains\n",
    "slightly different information but is not always populated with anything useful.\n",
    "\n",
    "We will likely see a similar pattern when comparing UA strings that appear in suspicious data with the\n",
    "successful logon data.\n",
    "\n",
    "> **Note** we use both the raw UserAgent string and extract version-agnostic versions<br>\n",
    "> of UA strings that have all of the numeric information zeroed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "SigninLogs\n",
    "| where TimeGenerated > datetime({wednesday}) and TimeGenerated < datetime({thursday})\n",
    "| where ResultType == 0\n",
    "| summarize ua_count=count() by UserAgent\n",
    "\"\"\"\n",
    "success_user_agents_df = qry_prov.exec_query(qry)\n",
    "\n",
    "print(\"Querying User Agent statistics...\")\n",
    "file_name = archive_name(\"success_user_agents_df\", wednesday, thursday)\n",
    "if not use_caching() or not archive_exists(file_name):\n",
    "    print(\"querying sample of current data...\")\n",
    "    success_user_agents_df = qry_prov.exec_query(qry)\n",
    "    save_archive(file_name, success_user_agents_df)\n",
    "else:\n",
    "    success_user_agents_df = get_archive(file_name)\n",
    "print(\"done.\")\n",
    "\n",
    "print(\"Extracting UserAgent version-agnostic patterns.\")\n",
    "print(\"Calculating usage statistics for User agents and UA pattern.\")\n",
    "# Calculate usage for User agents (calc for both raw UserAgent and UA pattern)\n",
    "success_user_agents_df = success_user_agents_df.rename(columns={\"count\": \"ua_count\"})\n",
    "success_user_agents_df[\"usage\"] = (\n",
    "    success_user_agents_df.ua_count / success_user_agents_df.ua_count.sum()\n",
    ")\n",
    "success_user_agents_df.head()\n",
    "\n",
    "# Remove numbers from UserAgent strings to remove version-specific data\n",
    "success_user_agents_df[\"ua_pattern\"] = success_user_agents_df.UserAgent.str.replace(\n",
    "    \"\\d\", \"x\"\n",
    ")\n",
    "\n",
    "if \"ua_pattern_usage\" in success_user_agents_df.columns:\n",
    "    success_user_agents_df.drop(columns=[\"ua_pattern_usage\"], inplace=True)\n",
    "success_user_agents_df = success_user_agents_df.merge(\n",
    "    success_user_agents_df[[\"ua_pattern\", \"usage\"]]\n",
    "    .groupby(\"ua_pattern\")\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"usage\": \"ua_pattern_usage\"}),\n",
    "    on=\"ua_pattern\",\n",
    ")\n",
    "\n",
    "failed_signin_list_df[\"ua_pattern\"] = failed_signin_list_df.UserAgent.str.replace(\n",
    "    \"\\d\", \"x\"\n",
    ")\n",
    "\n",
    "failed_ua_pattern_usage = (\n",
    "    failed_signin_list_df[[\"ua_pattern\"]]\n",
    "    .merge(\n",
    "        success_user_agents_df[[\"ua_pattern\", \"ua_pattern_usage\"]].drop_duplicates(),\n",
    "        on=\"ua_pattern\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "failed_signin_list_df[\"ua_pattern_usage\"] = failed_ua_pattern_usage[\"ua_pattern_usage\"]\n",
    "del failed_ua_pattern_usage\n",
    "\n",
    "np.histogram(failed_signin_list_df[\"ua_pattern_usage\"], bins=100)\n",
    "display(\n",
    "    failed_signin_list_df[[\"ua_pattern\", \"ua_pattern_usage\"]]\n",
    "    .drop_duplicates()[\"ua_pattern_usage\"]\n",
    "    .hvplot.hist(\n",
    "        bins=100,\n",
    "        title=\"Probability distribution for failed login User Agents - likelihood they use a known UA\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T18:01:45.297028Z",
     "start_time": "2020-08-24T18:01:45.265031Z"
    }
   },
   "outputs": [],
   "source": [
    "md(\n",
    "    \"UA strings in investigation data that <b>do not</b> appear in the success logon data (top 50)\",\n",
    "    \"large\",\n",
    ")\n",
    "cols = [\n",
    "    \"ResourceDisplayName\",\n",
    "    \"AppDisplayName\",\n",
    "    \"ResultType\",\n",
    "    \"ResultDescription\",\n",
    "    \"ua_pattern\",\n",
    "]\n",
    "obs_data = (\n",
    "    failed_signin_list_df[\n",
    "        ~failed_signin_list_df[\"ua_pattern\"].isin(success_user_agents_df.ua_pattern)\n",
    "    ][[*cols, \"TimeGenerated\"]]\n",
    "    .groupby(cols)\n",
    "    .count()\n",
    "    .rename(columns={\"TimeGenerated\": \"Login_attempts\"})\n",
    "    .sort_values(\"Login_attempts\", ascending=False)\n",
    "    .head(50)\n",
    "    .style.background_gradient(subset=[\"Login_attempts\"], cmap=\"Reds\")\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"UA strings in investigation data that **do not** appear in the success logon data (top 50)\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.5 UserAgent strings\"),\n",
    "    score=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T18:02:12.956259Z",
     "start_time": "2020-08-24T18:02:12.931264Z"
    }
   },
   "outputs": [],
   "source": [
    "md(\n",
    "    \"UA strings in investigation data that <b>do</b> appear in the success logon data (top 50)\",\n",
    "    \"large\",\n",
    ")\n",
    "obs_data = (\n",
    "    failed_signin_list_df[\n",
    "        failed_signin_list_df[\"ua_pattern\"].isin(success_user_agents_df[\"ua_pattern\"])\n",
    "    ][[*cols, \"TimeGenerated\"]]\n",
    "    .groupby(cols)\n",
    "    .count()\n",
    "    .rename(columns={\"TimeGenerated\": \"Login_attempts\"})\n",
    "    .sort_values(\"Login_attempts\", ascending=False)\n",
    "    .head(50)\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"UA strings in investigation data that **do** appear in the success logon data (top 50)\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.5 UserAgent strings\"),\n",
    "    score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "<hr>\n",
    "<p style='background-color: gold; font-size: medium'>\n",
    "    We should investigate any successful logins from accounts below with these user agents</p>\n",
    "Note: only data from failed logons data is shown here.\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T18:02:30.121589Z",
     "start_time": "2020-08-24T18:02:30.103589Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_data = (\n",
    "    failed_signin_list_df[\n",
    "        failed_signin_list_df[\"UserAgent\"].isin(success_user_agents_df[\"UserAgent\"])\n",
    "    ][\n",
    "        [\n",
    "            \"UserAgent\",\n",
    "            \"ua_pattern\",\n",
    "            \"ResultType\",\n",
    "            \"IPAddress\",\n",
    "            \"UserPrincipalName\",\n",
    "            \"TimeGenerated\",\n",
    "        ]\n",
    "    ]\n",
    "    .assign(UserPrincipalName=lambda x: mask_names(x.UserPrincipalName))\n",
    "    .groupby([\"ua_pattern\", \"ResultType\", \"UserPrincipalName\"])\n",
    "    .count()\n",
    "    .rename(columns={\"TimeGenerated\": \"Login_attempts\"})\n",
    "    .sort_values(\"Login_attempts\", ascending=False)\n",
    "    .head(40)\n",
    "    .style.background_gradient(subset=[\"Login_attempts\"], cmap=\"Reds\")\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"*User login failures with a UA that appears in the success logon data (top 40)\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.5 UserAgent strings\"),\n",
    "    score=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record observations in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_signin_list_df[\"UA_known\"] = failed_signin_list_df[\"ua_pattern\"].isin(\n",
    "    success_user_agents_df[\"ua_pattern\"]\n",
    ")\n",
    "\n",
    "failed_ua_pattern_usage = (\n",
    "    failed_signin_list_df[[\"ua_pattern\"]]\n",
    "    .merge(\n",
    "        success_user_agents_df[[\"ua_pattern\", \"ua_pattern_usage\"]].drop_duplicates(),\n",
    "        on=\"ua_pattern\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "failed_signin_list_df[\"ua_pattern_usage\"] = failed_ua_pattern_usage[\"ua_pattern_usage\"]\n",
    "del failed_ua_pattern_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Distinct UA-AppID pairing\n",
    "\n",
    "It is likely that different apps accessing email, documents, etc. will have different User Agent strings.\n",
    "E.g. a mail app may use a specific AppId and have a User Agent string specific to it.\n",
    "\n",
    "Here we are calculating the frequency at which User Agent strings are used with \n",
    "specific AppIds in successful logons.<br>\n",
    "We then compare our failed logons to see if their UserAgent/AppId pairings match,\n",
    "and if so, what the relative frequency a given pairing is found in success logons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "SigninLogs\n",
    "| where TimeGenerated > datetime({wednesday}) and TimeGenerated < datetime({thursday})\n",
    "| where ResultType == 0\n",
    "| summarize ua_appid_count=count() by UserAgent, AppId\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying UA-AppId statistics...\")\n",
    "file_name = archive_name(\"success_ua_appid_df\", wednesday, thursday)\n",
    "if not use_caching() or not archive_exists(file_name):\n",
    "    print(\"querying sample of current data...\")\n",
    "    success_ua_appid_df = qry_prov.exec_query(qry)\n",
    "    save_archive(file_name, success_ua_appid_df)\n",
    "else:\n",
    "    success_ua_appid_df = get_archive(file_name)\n",
    "print(\"done.\")\n",
    "\n",
    "success_ua_appid_df[\"ua_pattern\"] = success_ua_appid_df.UserAgent.str.replace(\"\\d\", \"x\")\n",
    "\n",
    "success_ua_appid_df = success_ua_appid_df.merge(\n",
    "    success_ua_appid_df[[\"ua_pattern\", \"ua_appid_count\"]]\n",
    "    .groupby(\"ua_pattern\")\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"ua_appid_count\": \"ua_ptn_appid_count\"}),\n",
    "    on=\"ua_pattern\",\n",
    ")\n",
    "success_ua_appid_df[\"ua_app_id_usage\"] = (\n",
    "    success_ua_appid_df[\"ua_appid_count\"] / success_ua_appid_df.ua_appid_count.sum()\n",
    ")\n",
    "success_ua_appid_df[\"ua_ptn_appid_usage\"] = (\n",
    "    success_ua_appid_df[\"ua_ptn_appid_count\"] / success_ua_appid_df.ua_appid_count.sum()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Merging into failed logins data\")\n",
    "# Merge usage stats into failed logons\n",
    "if \"ua_ptn_appid_usage\" in failed_signin_list_df.columns:\n",
    "    failed_signin_list_df.drop(columns=[\"ua_ptn_appid_usage\"], inplace=True)\n",
    "failed_ua_ptn_appid_df = failed_signin_list_df.merge(\n",
    "    success_ua_appid_df[\n",
    "        [\"ua_pattern\", \"AppId\", \"ua_ptn_appid_usage\"]\n",
    "    ].drop_duplicates(),\n",
    "    on=[\"ua_pattern\", \"AppId\"],\n",
    "    how=\"left\",\n",
    ").fillna(0)\n",
    "failed_signin_list_df[\"ua_ptn_appid_usage\"] = failed_ua_ptn_appid_df[\n",
    "    \"ua_ptn_appid_usage\"\n",
    "]\n",
    "display(\n",
    "    failed_signin_list_df[\"ua_ptn_appid_usage\"].hvplot.hist(\n",
    "        bins=100,\n",
    "        title=\"Failed logins - distribution using UA-AppId pairings found in success logins\",\n",
    "        color=\"orange\",\n",
    "        height=500,\n",
    "    )\n",
    "    + success_ua_appid_df.ua_ptn_appid_usage.hvplot.hist(\n",
    "        bins=100,\n",
    "        title=\"Success logins - distribution of UA-AppId pairings frequency\",\n",
    "        color=\"blue\",\n",
    "        height=500,\n",
    "    )\n",
    ")\n",
    "del failed_ua_ptn_appid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing top 30 failed logons and the frequency that the UA/AppID pairing was found in success logons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = (\n",
    "    failed_signin_list_df[\n",
    "        [\n",
    "            \"TimeGenerated\",\n",
    "            \"ua_pattern\",\n",
    "            \"AppId\",\n",
    "            \"UserPrincipalName\",\n",
    "            \"ua_ptn_appid_usage\",\n",
    "        ]\n",
    "    ]\n",
    "    .assign(UserPrincipalName=lambda x: mask_names(x.UserPrincipalName))\n",
    "    .groupby([\"ua_pattern\", \"AppId\", \"UserPrincipalName\", \"ua_ptn_appid_usage\"])\n",
    "    .count()\n",
    "    .rename(columns={\"TimeGenerated\": \"count\"})\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .head(30)\n",
    "    .style.format({\"ua_ptn_appid_usage\": \"{:.5f}\"})\n",
    "    .background_gradient(subset=[\"count\"], cmap=\"Reds\")\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Top login failures by UA pattern and UA/AppId usage (top 30)\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"Showing top 30 failed logons and the frequency that the UA/AppID pairing was found in success logons.\"),\n",
    "    score=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Use of legacy authentication protocols\n",
    "\n",
    "Modern apps and devices use either the 'Browser' or 'Mobile Apps and Desktop clients' protocols.\n",
    "\n",
    "AAD supports numerous legacy protocols, and are considered less secure.<br>\n",
    "They are also the protocols typically targeted by attackers since many do not support modern\n",
    "options such as multi-factor authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_app_usage = (\n",
    "    success_signin_list_df[[\"TimeGenerated\", \"ClientAppUsed\"]]\n",
    "    .groupby(\"ClientAppUsed\")\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"TimeGenerated\": \"cli_app_count\"})\n",
    "    .assign(client_app_usage=lambda x: x.cli_app_count / len(success_signin_list_df))\n",
    ")\n",
    "\n",
    "legacy_apps = legacy_clients()\n",
    "legacy_crit = failed_signin_list_df.ClientAppUsed.isin(legacy_apps)\n",
    "failed_signin_list_df.loc[legacy_crit, \"legacy_client\"] = True\n",
    "failed_signin_list_df.loc[~legacy_crit, \"legacy_client\"] = False\n",
    "\n",
    "\n",
    "if \"client_app_usage\" in failed_signin_list_df.columns:\n",
    "    failed_signin_list_df.drop(columns=[\"client_app_usage\"], inplace=True)\n",
    "failed_signin_list_df[\"client_app_usage\"] = (\n",
    "    failed_signin_list_df.merge(\n",
    "        client_app_usage[[\"ClientAppUsed\", \"client_app_usage\"]],\n",
    "        on=\"ClientAppUsed\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .client_app_usage\n",
    ")\n",
    "\n",
    "group_cols = [\n",
    "    \"ResultType\",\n",
    "    \"ResultDescription\",\n",
    "    \"UserAgent\",\n",
    "    \"AppDisplayName\",\n",
    "    \"ClientAppUsed\",\n",
    "    \"legacy_client\",\n",
    "    \"client_app_usage\",\n",
    "]\n",
    "obs_data = (\n",
    "    failed_signin_list_df[legacy_crit][[*group_cols, \"TimeGenerated\"]]\n",
    "    .groupby(group_cols)\n",
    "    .count()\n",
    "    .rename(columns={\"TimeGenerated\": \"count\"})\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"Top login failures by client app for legacy protocols\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.7 Use of legacy authentication protocols\"),\n",
    "    score=2,\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=(\n",
    "            failed_signin_list_df[legacy_crit].query(\"client_app_usage == 0.0\").IPAddress.unique()\n",
    "        ),\n",
    "        reason=\"Failed logons using legacy protocols not used in organization\",\n",
    "        section=\"2.7 Use of legacy authentication protocols\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 UA Pattern analysis\n",
    "\n",
    "TODO - we might be able to detect 'artificial' User Agent strings - i.e. UA strings being spoofed by malicious applications.\n",
    "\n",
    "- Take the UA pattern (i.e. the UA string with version digits removed)\n",
    "- Take a hash of the pattern - giving a unique value for each pattern\n",
    "- Take a combination of the following to enable us to match similar but not necessarily identical strings:\n",
    "  - the sum of ordinal character values of the UA string and rounded to nearest 10\n",
    "  - a count of the non-alphanumeric characters in the string (giving us a measure of the structure) (rounded to nearest 2)\n",
    "- Find all UAs that have an equal similarity but dissimilar hashes.\n",
    "- Do an edit-distance calculation to highlight the changes between the two UA versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.9 IP Addresses with multiple user names\n",
    "\n",
    "It should be unusual (VPNs/proxies aside) for many users to share an IP address.\n",
    "\n",
    "An IPAddress can easily have multiple UserAgents (think different apps on the same device) - so a high number is not necessarily a concern.\n",
    "In any case, this seems to be somewhat correlated with multiple users, per IP.\n",
    "\n",
    "We can group by IPAddress and get a count of unique users, useragents and result types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_users_grp = (\n",
    "    failed_signin_list_df[[\"IPAddress\", \"UserPrincipalName\", \"ResultType\", \"UserAgent\"]]\n",
    "    .assign(UserPrincipalName=lambda x: mask_names(x.UserPrincipalName))\n",
    "    .groupby(\"IPAddress\")\n",
    "    .agg(\n",
    "        ip_user_count=pd.NamedAgg(\"UserPrincipalName\", \"nunique\"),\n",
    "        user_sample=pd.NamedAgg(\"UserPrincipalName\", lambda x: x.unique().tolist()[:5]),\n",
    "        user_agent_sample=pd.NamedAgg(\"UserAgent\", lambda x: x.unique().tolist()[:5]),\n",
    "        user_agents=pd.NamedAgg(\"UserAgent\", \"nunique\"),\n",
    "        result_types=pd.NamedAgg(\"ResultType\", \"nunique\"),\n",
    "    )\n",
    ")\n",
    "multi_user_ips = (\n",
    "    ip_users_grp.query(\"ip_user_count > 1\").reset_index().IPAddress.unique()\n",
    ")\n",
    "md(f\"{len(multi_user_ips)} IPs with multiple users\", \"bold\")\n",
    "ips = sorted(list(mask_ips(pd.Series(multi_user_ips)).values))\n",
    "for idx in range(0, len(ips), 5):\n",
    "    for ip in ips[idx:idx + 5]:\n",
    "        print(ip, end=\", \")\n",
    "    print()\n",
    "\n",
    "md(\"<br>Top 20\", \"bold\")\n",
    "obs_data = (\n",
    "    ip_users_grp.query(\"ip_user_count > 1\")\n",
    "    .sort_values(\"ip_user_count\", ascending=False)\n",
    "    .head(20)\n",
    "    .reset_index()\n",
    "    .assign(IPAddress=lambda x: mask_ips(x.IPAddress))\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"*IP Addresses with multiple user names (top 20)\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.9 IP Addresses with multiple user names\"),\n",
    "    score=2,\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=multi_user_ips,\n",
    "        reason=\"IP Addresses with multiple user names\",\n",
    "        section=\"2.9 IP Addresses with multiple user names\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Add this data to our Failed user DataFrame\n",
    "if \"ip_user_count\" in failed_signin_list_df.columns:\n",
    "    failed_signin_list_df.drop(columns=[\"ip_user_count\"], inplace=True)\n",
    "\n",
    "failed_signin_list_df = failed_signin_list_df.merge(\n",
    "    ip_users_grp.reset_index()[[\"IPAddress\", \"ip_user_count\"]], on=\"IPAddress\"\n",
    ")\n",
    "del ip_users_grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.10 Users with multiple IPs, UserAgents, Locations\n",
    "\n",
    "This is potentially interesting but it might be common to see users with multiple IP addresses, user agents and often locations.\n",
    "\n",
    "Unless you see something unusual in the top scorers\n",
    "for unique distinct IP Addresses, Locations, UserAgents or ResultTypes, we won't use this data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ips_grp_clr = (\n",
    "    failed_signin_list_df[\n",
    "        [\n",
    "            \"IPAddress\",\n",
    "            \"IPBlock_str\",\n",
    "            \"UserPrincipalName\",\n",
    "            \"ResultType\",\n",
    "            \"UserAgent\",\n",
    "            \"Location\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby(\"UserPrincipalName\")\n",
    "    .agg(\n",
    "        ip_count=pd.NamedAgg(\"IPAddress\", \"nunique\"),\n",
    "        ip_sample=pd.NamedAgg(\"IPAddress\", lambda x: x.unique().tolist()[:5]),\n",
    "        country_count=pd.NamedAgg(\"Location\", \"nunique\"),\n",
    "        country_sample=pd.NamedAgg(\"Location\", lambda x: x.unique().tolist()[:5]),\n",
    "        user_agents=pd.NamedAgg(\"UserAgent\", \"nunique\"),\n",
    "        user_agent_sample=pd.NamedAgg(\"UserAgent\", lambda x: x.unique().tolist()[:5]),\n",
    "        result_types=pd.NamedAgg(\"ResultType\", \"nunique\"),\n",
    "        result_type_sample=pd.NamedAgg(\"ResultType\", lambda x: x.unique().tolist()[:5]),\n",
    "    )\n",
    ")\n",
    "\n",
    "def mask_ip_list(series):\n",
    "    return series.apply(lambda x: [hash_ip(ip) for ip in x])\n",
    "\n",
    "\n",
    "user_ips_grp = user_ips_grp_clr.reset_index().assign(\n",
    "    UserPrincipalName=lambda x: mask_names(x.UserPrincipalName),\n",
    "    ip_sample=lambda x: mask_ip_list(x.ip_sample),\n",
    ")\n",
    "md(\"By ip_count\", \"bold\")\n",
    "display(user_ips_grp.sort_values(\"ip_count\", ascending=False).head(5))\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logins by ip_count\",\n",
    "    data=user_ips_grp.sort_values(\"ip_count\", ascending=False).head(5),\n",
    "    link=fmt_link(\"2.10 Users with multiple IPs, UserAgents, Locations\"),\n",
    "    score=1,\n",
    ")\n",
    "md(\"By country_count\", \"bold\")\n",
    "display(user_ips_grp.sort_values(\"country_count\", ascending=False).head(5))\n",
    "obs_log.add_observation(\n",
    "    caption=\"Failed logins by country_count\",\n",
    "    data=user_ips_grp.sort_values(\"country_count\", ascending=False).head(5),\n",
    "    link=fmt_link(\"2.10 Users with multiple IPs, UserAgents, Locations\"),\n",
    "    score=1,\n",
    ")\n",
    "del user_ips_grp_clr, user_ips_grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 IP Addresses Identified as Malicious by Azure Active Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_ip_desc = (\n",
    "    \"Sign-in was blocked because it came from an IP address with malicious activity.\"\n",
    ")\n",
    "mal_blocked_ips = failed_signin_list_df[\n",
    "    [\n",
    "        \"IPAddress\",\n",
    "        \"UserPrincipalName\",\n",
    "        \"ResultType\",\n",
    "        \"ResultDescription\",\n",
    "        \"UserAgent\",\n",
    "        \"Location\",\n",
    "    ]\n",
    "].query(\"ResultDescription == @mal_ip_desc\")\n",
    "\n",
    "obs_data = (\n",
    "    mal_blocked_ips.assign(\n",
    "        UserPrincipalName=lambda x: mask_names(x.UserPrincipalName),\n",
    "        IPAddress=lambda x: mask_ips(x.IPAddress),\n",
    "    )\n",
    "    .groupby(\"IPAddress\")\n",
    "    .agg(\n",
    "        ip_user_count=pd.NamedAgg(\"UserPrincipalName\", \"nunique\"),\n",
    "        user_sample=pd.NamedAgg(\"UserPrincipalName\", lambda x: x.unique().tolist()[:5]),\n",
    "        user_agent_sample=pd.NamedAgg(\"UserAgent\", lambda x: x.unique().tolist()[:5]),\n",
    "        user_agents=pd.NamedAgg(\"UserAgent\", \"nunique\"),\n",
    "        result_types=pd.NamedAgg(\"ResultType\", \"nunique\"),\n",
    "    )\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"IP Addresses blocked by AAD because malicious\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"2.11 IP Addresses Identified as Malicious by Azure Active Directory\"),\n",
    "    score=2,\n",
    ")\n",
    "ip_log.extend(\n",
    "    create_ip_entries(\n",
    "        ip_addresses=mal_blocked_ips.IPAddress.unique(),\n",
    "        reason=\"IP Addresses blocked by AAD because malicious\",\n",
    "        section=\"2.11 IP Addresses Identified as Malicious by Azure Active Directory\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Add this data to our Failed user DataFrame\n",
    "if \"known_malicious\" in failed_signin_list_df.columns:\n",
    "    failed_signin_list_df.drop(columns=[\"known_malicious\"], inplace=True)\n",
    "\n",
    "failed_signin_list_df = failed_signin_list_df.merge(\n",
    "    mal_blocked_ips[[\"IPAddress\"]].assign(known_malicious=True),\n",
    "    on=\"IPAddress\",\n",
    "    how=\"left\",\n",
    ").fillna(False)\n",
    "\n",
    "del mal_blocked_ips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3 - Data Enrichment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Threat Intel Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msticpy.sectools import TILookup\n",
    "\n",
    "# Create a TILookup instance - this caches lookups, so don't recreate on each run.\n",
    "ti_lookup = TILookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Change these two variables if you have data limits on your TI providers\n",
    "# e.g. 4 lookups per minute - SLICE_SIZE = 4, SLEEP_SECS = 55\n",
    "SLICE_SIZE = 10 \n",
    "SLEEP_SECS = 1\n",
    "\n",
    "# Get all IPs with > 1 reason for inclusion in observations\n",
    "suspicious_ips = (\n",
    "    pd.DataFrame(data=set(ip_log))\n",
    "    .groupby(\"address\")\n",
    "    .agg({\"reason\": \"nunique\"})\n",
    "    .sort_values(\"reason\", ascending=False)\n",
    "    .query(\"reason > 1\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Looking up TI Results...\")\n",
    "ip_slices = [\n",
    "    suspicious_ips.iloc[idx : idx + SLICE_SIZE]\n",
    "    for idx in range(0, len(suspicious_ips), SLICE_SIZE)\n",
    "]\n",
    "result_slices = []\n",
    "for ip_slice in tqdm(ip_slices, desc=\"IoC slices\"):\n",
    "    result_slices.append(ti_lookup.lookup_iocs(data=ip_slice, obs_col=\"address\"))\n",
    "    print(f\"\\nWaiting {SLEEP_SECS} sec for next batch\", end=\"\")\n",
    "    for sec in range(SLEEP_SECS):\n",
    "        print(\".\", end=\"\")\n",
    "        sleep(1)\n",
    "\n",
    "ti_results = pd.concat(result_slices)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_results[\"ti_severity\"] = ti_results.Severity.map(\n",
    "    {\"high\": 5, \"warning\": 3, \"information\": 1}\n",
    ")\n",
    "ti_results_grouped = ti_results.groupby(\"Ioc\").agg(\n",
    "    providers=pd.NamedAgg(\"Provider\", lambda x: x.unique().tolist()),\n",
    "    details=pd.NamedAgg(\"Details\", lambda x: x.tolist()),\n",
    "    references=pd.NamedAgg(\"Reference\", lambda x: x.unique().tolist()),\n",
    "    max_severity=pd.NamedAgg(\"ti_severity\", \"max\"),\n",
    ").reset_index()\n",
    "failed_signins_ti = (\n",
    "    failed_signin_list_df.merge(\n",
    "        ti_results_grouped[[\"Ioc\", \"providers\", \"details\", \"references\", \"max_severity\"]], \n",
    "        left_on=\"IPAddress\",\n",
    "        right_on=\"Ioc\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .fillna({\"max_severity\": 0})\n",
    ")\n",
    "\n",
    "failed_signin_list_df[\"max_ti_severity\"] = failed_signins_ti.max_severity\n",
    "\n",
    "obs_data = (\n",
    "    failed_signin_list_df\n",
    "    [[\n",
    "        \"IPAddress\",\n",
    "        \"UserPrincipalName\",\n",
    "        \"ResultType\",\n",
    "        \"ResultDescription\",\n",
    "        \"UserAgent\",\n",
    "        \"Location\",\n",
    "        \"max_ti_severity\"\n",
    "    ]]\n",
    "    .merge(\n",
    "        ti_results_grouped[[\"Ioc\", \"providers\", \"details\", \"references\"]], \n",
    "        left_on=\"IPAddress\",\n",
    "        right_on=\"Ioc\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .assign(\n",
    "        UserPrincipalName=lambda x: mask_names(x.UserPrincipalName),\n",
    "        IPAddress=lambda x: mask_ips(x.IPAddress),\n",
    "    )\n",
    "    .groupby([\"IPAddress\", \"UserAgent\"])\n",
    "    .agg(\n",
    "        user_count=pd.NamedAgg(\"UserPrincipalName\", \"nunique\"),\n",
    "        logins_count=pd.NamedAgg(\"UserPrincipalName\", \"count\"),\n",
    "        location_count=pd.NamedAgg(\"Location\", \"nunique\"),\n",
    "        max_ti_severity=pd.NamedAgg(\"max_ti_severity\", \"max\"),\n",
    "        result_types_count=pd.NamedAgg(\"ResultType\", \"nunique\"),\n",
    "        providers=pd.NamedAgg(\"providers\", \"first\"),\n",
    "        references=pd.NamedAgg(\"references\", lambda x: x.tolist()),\n",
    "    )\n",
    ")\n",
    "display(obs_data)\n",
    "obs_log.add_observation(\n",
    "    caption=\"*IP Addresses with postive Threat Intel results\",\n",
    "    data=obs_data,\n",
    "    link=fmt_link(\"3.1 Threat Intel Lookup\"),\n",
    "    score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4 - Bringing the data together\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msticpy.sectools.eventcluster import (\n",
    "    char_ord_score_df,\n",
    "    crc32_hash_df,\n",
    "    delim_count_df,\n",
    ")\n",
    "\n",
    "# Create features\n",
    "source_cols = [\n",
    "    \"ResultType\",\n",
    "    \"UserAgent\",\n",
    "    \"AppId\",\n",
    "    \"scheduled\",\n",
    "    \"known_ip_block\",\n",
    "    \"UA_known\",\n",
    "    \"login_loc_prob\",\n",
    "    \"device_str\",\n",
    "    \"ip_user_count\",\n",
    "    \"ua_pattern_usage\",\n",
    "    \"ua_pattern\",\n",
    "    \"ua_ptn_appid_usage\",\n",
    "    \"legacy_client\",\n",
    "    \"client_app_usage\",\n",
    "    \"max_ti_severity\",\n",
    "    \"known_malicious\",\n",
    "]\n",
    "failed_logins_features = failed_signin_list_df[source_cols].copy()\n",
    "# failed_logins_features[\"result_id\"]\n",
    "failed_logins_features[\"result_type\"] = failed_logins_features[\"ResultType\"].astype(\n",
    "    \"int32\"\n",
    ")\n",
    "failed_logins_features[\"user_agent\"] = failed_logins_features[\"UserAgent\"].astype(\n",
    "    \"category\"\n",
    ")\n",
    "failed_logins_features[\"user_agent_scr\"] = char_ord_score_df(\n",
    "    failed_logins_features, \"UserAgent\"\n",
    ")\n",
    "failed_logins_features[\"user_agent_struc\"] = delim_count_df(\n",
    "    failed_logins_features, \"UserAgent\"\n",
    ")\n",
    "failed_logins_features[\"user_agent_ptn_scr\"] = char_ord_score_df(\n",
    "    failed_logins_features, \"ua_pattern\"\n",
    ")\n",
    "failed_logins_features[\"app_id_scr\"] = char_ord_score_df(\n",
    "    failed_logins_features, \"AppId\"\n",
    ")\n",
    "failed_logins_features[\"device_str\"] = char_ord_score_df(\n",
    "    failed_logins_features, \"device_str\"\n",
    ")\n",
    "# failed_logins_features[\"device_str_struc\"] = delim_count_df(failed_logins_features, \"device_str\")\n",
    "\n",
    "input_cols = [\n",
    "    \"scheduled\",\n",
    "    \"known_ip_block\",\n",
    "    \"UA_known\",\n",
    "    \"login_loc_prob\",\n",
    "    \"user_agent_scr\",\n",
    "    \"user_agent_struc\",\n",
    "    \"app_id_scr\",\n",
    "    \"device_str\",\n",
    "    \"ip_user_count\",\n",
    "    \"user_agent_ptn_scr\",\n",
    "    \"ua_pattern_usage\",\n",
    "    \"ua_ptn_appid_usage\",\n",
    "    \"legacy_client\",\n",
    "    \"client_app_usage\",\n",
    "    \"max_ti_severity\",\n",
    "    \"known_malicious\",\n",
    "]\n",
    "default_cols = [\n",
    "    \"known_ip_block\",\n",
    "    \"login_loc_prob\",\n",
    "    \"user_agent_struc\",\n",
    "    \"ua_pattern_usage\",\n",
    "    \"ua_ptn_appid_usage\",\n",
    "    \"legacy_client\",\n",
    "    \"client_app_usage\",\n",
    "    \"max_ti_severity\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select cluster parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_layout = widgets.Layout(width=\"50%\")\n",
    "w_style = {\"description_width\": \"200px\"}\n",
    "cluster_strength = widgets.FloatSlider(\n",
    "    description=\"Cluster strength\",\n",
    "    min=0.1,\n",
    "    max=5.0,\n",
    "    value=1.5,\n",
    "    step=0.1,\n",
    "    layout=w_layout,\n",
    "    style=w_style,\n",
    ")\n",
    "\n",
    "min_cluster_size = widgets.IntSlider(\n",
    "    description=\"Min cluster size\",\n",
    "    min=1,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    value=10,\n",
    "    layout=w_layout,\n",
    "    style=w_style,\n",
    ")\n",
    "\n",
    "cluster_props = widgets.SelectMultiple(\n",
    "    description=\"Cluster Properties\",\n",
    "    options=input_cols,\n",
    "    value=default_cols,\n",
    "    layout=widgets.Layout(width=\"50%\", height=\"200px\"),\n",
    "    style=w_style,\n",
    ")\n",
    "cluster_datasets = widgets.SelectMultiple(\n",
    "    description=\"Datasets\",\n",
    "    options=failed_logins_features.ResultType.unique(),\n",
    "    value=list(failed_logins_features.ResultType.unique()),\n",
    "    layout=widgets.Layout(width=\"50%\", height=\"200px\"),\n",
    "    style=w_style,\n",
    ")\n",
    "display(cluster_strength)\n",
    "display(min_cluster_size)\n",
    "display(cluster_props)\n",
    "display(cluster_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run clustering algorithm (DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "failed_logins_x = failed_logins_features[\n",
    "    failed_logins_features.ResultType.isin(cluster_datasets.value)\n",
    "]\n",
    "failed_logins_data_x = failed_signin_list_df[\n",
    "    failed_signin_list_df.ResultType.isin(cluster_datasets.value)\n",
    "]\n",
    "X = failed_logins_x[list(cluster_props.value)]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "print(\"Fitting cluster...\")\n",
    "db = DBSCAN(eps=cluster_strength.value, min_samples=min_cluster_size.value).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Used to normalize these two scores\n",
    "max_known_ip_block = failed_logins_x[\"known_ip_block\"].max()\n",
    "max_known_ua_usage = failed_logins_x[\"ua_pattern_usage\"].max()\n",
    "max_ip_user_count = failed_logins_x[\"ip_user_count\"].max()\n",
    "\n",
    "cluster_groups = []\n",
    "for label in unique_labels:\n",
    "    cluster_group = pd.Series()\n",
    "    cluster_group.name = label\n",
    "\n",
    "    failed_df_x = failed_logins_x[labels == label]\n",
    "    cluster_group[\"app_ids\"] = \"\\n\".join(list(failed_df_x[\"AppId\"].unique()))\n",
    "    cluster_group[\"user_agents\"] = \"\\n\".join(list(failed_df_x[\"ua_pattern\"].unique()))\n",
    "    cluster_group[\"num_users\"] = failed_logins_data_x[labels == label][\n",
    "        \"UserPrincipalName\"\n",
    "    ].nunique()\n",
    "    cluster_group[\"num_user_agents\"] = failed_df_x[\"ua_pattern\"].nunique()\n",
    "    cluster_group[\"num_ap_ids\"] = failed_df_x[\"AppId\"].nunique()\n",
    "    cluster_group[\"num_ips\"] = failed_logins_data_x[labels == label].IPAddress.nunique()\n",
    "    cluster_group[\"cluster_size\"] = len(failed_df_x)\n",
    "\n",
    "    cluster_group[\"med_known_ip_blk\"] = (\n",
    "        failed_df_x[\"known_ip_block\"].median() / max_known_ip_block\n",
    "    )\n",
    "    cluster_group[\"med_loc_prob\"] = failed_df_x[\"login_loc_prob\"].median()\n",
    "    cluster_group[\"med_scheduled\"] = failed_df_x[\"scheduled\"].median()\n",
    "    cluster_group[\"med_ip_user_count\"] = failed_df_x[\"ip_user_count\"].median()\n",
    "    cluster_group[\"med_ua_known\"] = failed_df_x[\"UA_known\"].median()\n",
    "    cluster_group[\"ua_pattern_usage\"] = failed_df_x[\"ua_pattern_usage\"].median()\n",
    "    cluster_group[\"ua_ptn_appid_usage\"] = failed_df_x[\"ua_ptn_appid_usage\"].median()\n",
    "    cluster_group[\"client_app_usage\"] = failed_df_x[\"client_app_usage\"].median()\n",
    "    cluster_group[\"legacy_client\"] = failed_df_x[\"legacy_client\"].median()\n",
    "    cluster_group[\"max_ti_severity\"] = failed_df_x[\"max_ti_severity\"].median()\n",
    "\n",
    "    add_scores = (\n",
    "        cluster_group[\"med_known_ip_blk\"]  # smaller is more suspicious\n",
    "        + cluster_group[\"med_loc_prob\"]  # smaller is more suspicious\n",
    "        + (1 - cluster_group[\"med_scheduled\"])  # 1 is more suspicious\n",
    "        # + (1 - cluster_group[\"med_ip_user_count\"])  # larger is more suspicious\n",
    "        # + cluster_group[\"med_ua_known\"]  # 0 is more suspicious\n",
    "        + cluster_group[\"ua_ptn_appid_usage\"]\n",
    "        + cluster_group[\"client_app_usage\"]\n",
    "        + cluster_group[\"ua_pattern_usage\"]  # smaller is more suspicious\n",
    "        + cluster_group[\"max_ti_severity\"]\n",
    "        + 1 - cluster_group[\"legacy_client\"]\n",
    "    )\n",
    "    #     print(\n",
    "    #         cluster_group[\"med_known_ip_blk\"],  # smaller is more suspicious\n",
    "    #         cluster_group[\"med_loc_prob\"],  # smaller is more suspicious\n",
    "    #         #         (1 - cluster_group[\"med_scheduled\"]),  # 1 is more suspicious\n",
    "    #         #         (1 - cluster_group[\"med_ip_user_count\"]),  # larger is more suspicious\n",
    "    #         cluster_group[\"med_ua_known\"],  # 0 is more suspicious\n",
    "    #         cluster_group[\"ua_ptn_appid_usage\"],\n",
    "    #         cluster_group[\"client_app_usage\"],\n",
    "    #         cluster_group[\"ua_pattern_usage\"]\n",
    "    #     )\n",
    "    add_scores = add_scores if add_scores else 1\n",
    "    cluster_group[\"combined_score\"] = 1 / add_scores\n",
    "    #     print(min(cluster_group[\"combined_score\"], 10))\n",
    "\n",
    "    cluster_groups.append(cluster_group)\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display overall stats for the clusters\n",
    "\n",
    "> **Note** the `combined_score` is a rough approximation.<br>\n",
    "> It is mostly based on the known locations, known ip blocks and known user agent data.<br>\n",
    "> In most cases this combination will indicate the most suspicious clusters but \n",
    "> there may be other overriding factors (e.g. an attack from a known location).<br>\n",
    "> You should examine the data carefully to determine the possible threat posed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cluster_df.sort_values(\"combined_score\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of clusters - clusters nearer zero are more suspicious\n",
    "\n",
    "- Vertical axis is the median probability of the logon IP coming from a known location\n",
    "- Horizontal axis is the median probability of the logon IP belonging to a know IP block\n",
    "- Circle size represents the cluster size\n",
    "- Color is combined score - deeper color == higher \"suspiciousness\" score.\n",
    "\n",
    "> Note: multiple clusters may occupy the same place on the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.hvplot.scatter(\n",
    "    x=\"med_known_ip_blk\",\n",
    "    y=\"med_loc_prob\",\n",
    "    size=\"cluster_size\",\n",
    "    color=\"combined_score\",\n",
    "    cmap=\"reds\",\n",
    "    line_color=\"black\",\n",
    "    height=500,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Browse clustered events\n",
    "\n",
    "The browser below groups related events together making it easier to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_select_options = [\n",
    "    (\n",
    "        label,\n",
    "        cluster_df.loc[label].cluster_size,\n",
    "        cluster_df.loc[label].combined_score,\n",
    "        cluster_df.loc[label].num_users,\n",
    "    )\n",
    "    for label in unique_labels\n",
    "]\n",
    "clus_select_options = sorted(clus_select_options, key=lambda x: x[2], reverse=True)\n",
    "option_dict = {\n",
    "    (\n",
    "        f\"Cluster {item[0]:2}, \"\n",
    "        + f\" size: {item[1]:4}, \"\n",
    "        + f\" score: {item[2]:6.2f}, \"\n",
    "        + f\" users: {item[3]:3}\"\n",
    "    ): item[0]\n",
    "    for item in clus_select_options\n",
    "}\n",
    "\n",
    "select_cluster = widgets.Select(\n",
    "    options=option_dict,\n",
    "    description=\"Select cluster to view\",\n",
    "    layout=widgets.Layout(width=\"50%\", height=\"200px\"),\n",
    "    style={\"description_width\": \"150px\"},\n",
    ")\n",
    "\n",
    "group_cols = [\n",
    "    \"IPAddress\",\n",
    "    \"Location\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"ResultDescription\",\n",
    "    \"AppDisplayName\",\n",
    "    \"ua_pattern\",\n",
    "    \"UA_known\",\n",
    "    \"known_ip_block\",\n",
    "    \"scheduled\",\n",
    "    \"login_loc_prob\",\n",
    "    \"ua_pattern_usage\",\n",
    "    \"max_ti_severity\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_grouped_data(label):\n",
    "    return (\n",
    "        failed_logins_data_x[labels == label][[\"TimeGenerated\", *group_cols]]\n",
    "        .groupby(group_cols)\n",
    "        .agg(\n",
    "            start=pd.NamedAgg(\"TimeGenerated\", \"min\"),\n",
    "            end=pd.NamedAgg(\"TimeGenerated\", \"max\"),\n",
    "            events=pd.NamedAgg(\"TimeGenerated\", \"count\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"events\", ascending=False)[[\"start\", \"end\", \"events\", *group_cols]]\n",
    "    )\n",
    "\n",
    "\n",
    "def disp_header(label):\n",
    "    if label == -1:\n",
    "        disp_head_id.update(Markdown(\"**Unclustered events/noise points**\"))\n",
    "    else:\n",
    "        disp_head_id.update(Markdown(\n",
    "            f\"**Cluster {label}   Score {cluster_df.loc[label].combined_score:0.3f}**\"\n",
    "        ))\n",
    "\n",
    "\n",
    "def show_logins(change):\n",
    "    label = change.get(\"new\")\n",
    "    disp_header(label)\n",
    "    if disp_df_id:\n",
    "        disp_stat_id.update(pd.DataFrame(cluster_df.loc[label]).T)\n",
    "        disp_df_id.update(get_grouped_data(label))\n",
    "\n",
    "\n",
    "md(\"Note - cluster with label '-1' is the group of remaining unclustered events.\")\n",
    "select_cluster.observe(show_logins, names=\"value\")\n",
    "display(select_cluster)\n",
    "\n",
    "first_label = next(iter(option_dict.values()))\n",
    "disp_head_id = display(\"\", display_id=True)\n",
    "disp_header(first_label)\n",
    "disp_stat_id = display(pd.DataFrame(cluster_df.loc[first_label]), display_id=True)\n",
    "disp_df_id = display(get_grouped_data(first_label), display_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Browse investigation observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = list(obs_log.observation_list.keys())\n",
    "select_obs = widgets.Select(options=opts, style=style, layout=layout)\n",
    "\n",
    "\n",
    "def display_observation(obs, disp_ids=None):\n",
    "    \"\"\"Display the observation.\"\"\"\n",
    "    desc = obs.description or \"\"\n",
    "    link = f\"[Go to notebook section: *{obs.link}*](#{obs.link})\" if obs.link else \"\"\n",
    "    if disp_ids is None:\n",
    "        disp_ids = [\n",
    "            display(Markdown(f\"### {obs.caption}\"), display_id=True),\n",
    "            display(Markdown(desc), display_id=True),\n",
    "            display(Markdown(f\"Score: {obs.score}\"), display_id=True),\n",
    "            display(Markdown(link), display_id=True),\n",
    "            display(obs.data, display_id=True),\n",
    "            display(Markdown(\"---\"), display_id=True),\n",
    "        ]\n",
    "    else:\n",
    "        disp_ids[0].update(Markdown(f\"### {obs.caption}\"))\n",
    "        disp_ids[1].update(Markdown(desc))\n",
    "        disp_ids[2].update(Markdown(f\"Score: {obs.score}\"))\n",
    "        disp_ids[3].update(Markdown(link))\n",
    "        disp_ids[4].update(obs.data)\n",
    "        disp_ids[5].update(Markdown(\"---\"))\n",
    "    return disp_ids\n",
    "\n",
    "\n",
    "def disp_obs(change):\n",
    "    obs_key = change.get(\"new\")\n",
    "    display_observation(obs_log.observation_list.get(obs_key), disp_ids)\n",
    "\n",
    "\n",
    "select_obs.observe(disp_obs, names=\"value\")\n",
    "display(select_obs)\n",
    "disp_ids = display_observation(next(iter(obs_log.observation_list.values())))\n",
    "select_obs.value = next(iter(obs_log.observation_list.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Results query browser\n",
    "\n",
    "This section lets you display the full results of the signin failures. You can query on \n",
    "any of the DataFrame properties, remove duplicates and select subsets of columns to view.\n",
    "\n",
    "Type in a query expression into the `Data query` box and hit the `Apply query` button.\n",
    "- strings must be quoted\n",
    "- you can use and and or to group sub-expressions\n",
    "- for string operations (e.g. \"contains\") use the pandas `str` accessor. E.g.\n",
    "  - UserPrincipalName.str.contains(\"ian\")\n",
    "  \n",
    "The `Avail columns` control (from the left) is just a list of columns - \n",
    "making it easier to copy/paste column names into a query.<br>\n",
    "The `Show columns` control lets you select which columns to display.<br>\n",
    "`Apply` applies the current query (if any), column selection, duplicate removal and Top N settings.<br>\n",
    "`Show top` restricts the output to the top N rows. 0 means all rows.<br>\n",
    "`Drop duplicates` tries to remove duplicate columns. In cases where the column value \n",
    "(e.g. LocationDetails) is a dictionary or other unhashable type this will not work.<br>\n",
    "`Results to clipboard` copies a tab-separated list of the current results to the clipboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_style = {\"description_width\": \"100px\"}\n",
    "right_align = widgets.Layout(align_self=\"center\")\n",
    "query_text = widgets.Textarea(\n",
    "    description=\"Data query\",\n",
    "    layout=widgets.Layout(width=\"70%\", height=\"100px\"),\n",
    "    style=w_style,\n",
    ")\n",
    "col_names = widgets.Textarea(\n",
    "    description=\"Avail columns\",\n",
    "    value=\"\\n\".join(sorted(failed_signin_list_df.columns)),\n",
    "    placeholder=\"max_ti_severity == 5\",\n",
    "    layout=widgets.Layout(width=\"30%\", height=\"150px\"),\n",
    "    style=w_style,\n",
    "    disabled=True,\n",
    ")\n",
    "col_select = widgets.SelectMultiple(\n",
    "    description=\"Show columns\",\n",
    "    options=list(sorted(failed_signin_list_df.columns)),\n",
    "    value=list(sorted(failed_signin_list_df.columns)),\n",
    "    layout=widgets.Layout(width=\"40%\", height=\"150px\"),\n",
    "    style=w_style,\n",
    ")\n",
    "head_len = widgets.BoundedIntText(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(failed_signin_list_df),\n",
    "    description=\"Show top\",\n",
    "    layout=widgets.Layout(align_self=\"center\", width=\"200px\"),\n",
    "    style=w_style,\n",
    ")\n",
    "apply_btn = widgets.Button(description=\"Apply query\", layout=right_align)\n",
    "copy_btn = widgets.Button(description=\"Results to clipboard\", layout=right_align)\n",
    "no_dups = widgets.Checkbox(description=\"Drop duplicates\", value=True, layout=right_align)\n",
    "\n",
    "\n",
    "def get_filtered_events():\n",
    "    df_display = failed_signin_list_df\n",
    "    if query_text.value:\n",
    "        qry = query_text.value.replace(\"\\n\", \" \")\n",
    "        df_display = df_display.query(qry)\n",
    "    if head_len.value > 0:\n",
    "        df_display = df_display.head(head_len.value)\n",
    "    df_display = df_display[list(col_select.value)]\n",
    "    if no_dups.value:\n",
    "        try:\n",
    "            df_display = df_display.drop_duplicates()\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return df_display\n",
    "\n",
    "\n",
    "def filter_events(change):\n",
    "    del change\n",
    "    df_display = get_filtered_events()\n",
    "    disp_filt_df_id.update(df_display)\n",
    "\n",
    "\n",
    "def to_clipboard(change):\n",
    "    del change\n",
    "    get_filtered_events().to_clipboard()\n",
    "\n",
    "\n",
    "apply_btn.on_click(filter_events)\n",
    "copy_btn.on_click(to_clipboard)\n",
    "actions = widgets.HBox(children=[apply_btn, no_dups, head_len, copy_btn])\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([query_text, col_names]),\n",
    "            col_select,\n",
    "            actions,\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "md(\"<hr>\")\n",
    "disp_filt_df_id = display(failed_signin_list_df.head(10), display_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br><br><br><br><br>\n",
    "# ----- End of Part 4 ------\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def show_mem_usage(namespace, df_only=True, thresh_kb=100):\n",
    "    vars = list(namespace.keys())\n",
    "    df_memory = []\n",
    "    for name in vars:\n",
    "        val = namespace.get(name)\n",
    "        if df_only and isinstance(val, pd.DataFrame):\n",
    "            df_memory.append((name, type(val).__name__, val.memory_usage().sum()))\n",
    "        else:\n",
    "            df_memory.append((name, type(val).__name__, sys.getsizeof(val, 666)))\n",
    "\n",
    "    for name, v_type, mem_usage in sorted(df_memory, key=lambda x: x[2], reverse=True):\n",
    "        if mem_usage > thresh_kb * 1024:\n",
    "            print(f\"{name:40} {v_type:40} {mem_usage /  1024:>20,.1f} kb\")\n",
    "\n",
    "\n",
    "show_mem_usage(globals(), df_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "data_path = f\"pwd_spray_data{datetime.now().strftime('%Y-%m-%dT%H-%M')}\"\n",
    "\n",
    "if not Path(data_path).exists():\n",
    "    Path(data_path).mkdir()\n",
    "print(\"Saving dataFrames in Notebook\")\n",
    "print(\"-\" * 50)\n",
    "current_vars = list(locals().keys())\n",
    "for var_name in current_vars:\n",
    "    if isinstance(locals()[var_name], pd.DataFrame) and not var_name.startswith(\"_\"):\n",
    "        print(\"saving \", data_path + \"/\" + var_name + \".pkl\")\n",
    "        try:\n",
    "            locals()[var_name].to_pickle(data_path + \"/\" + var_name + \".pkl\")\n",
    "        except MemoryError:\n",
    "            print(\"Failed to save\", var_name)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "history": [],
  "kernelspec": {
   "display_name": "Python (condadev)",
   "language": "python",
   "name": "condadev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-showtags": false,
  "uuid": "126eb663-e029-4e12-9f21-fc188fcb4c71",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
